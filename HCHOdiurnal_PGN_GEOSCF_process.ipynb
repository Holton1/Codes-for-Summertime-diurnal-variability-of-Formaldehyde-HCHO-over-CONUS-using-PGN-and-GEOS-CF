{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "429daec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libs\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "import xarray as xr\n",
    "from netCDF4 import Dataset\n",
    "import xesmf as xe\n",
    "import sys\n",
    "import warnings\n",
    "import cmaps\n",
    "warnings.filterwarnings('ignore')\n",
    "import glob\n",
    "import cartopy.feature as cfeature\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\n",
    "import matplotlib.cm as cm\n",
    "import pytz\n",
    "\n",
    "# import rum\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib as mpl\n",
    "\n",
    "\n",
    "mpl.rcParams['font.sans-serif'] = \"DejaVu Sans\"\n",
    "mpl.rcParams['font.family'] = \"sans-serif\"\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "mpl.rc('xtick', labelsize=18)\n",
    "mpl.rc('ytick', labelsize=18)\n",
    "plt.rcParams['axes.labelsize'] = 18\n",
    "plt.rcParams['axes.titlesize'] = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b59f7680",
   "metadata": {},
   "outputs": [],
   "source": [
    "jndir = \"/import/GREENING/tzhao/jndata/TROPOMI_HCHO/\"\n",
    "jndirGC = \"/import/GREENING/tzhao/jndata/GEOS-Chem/\"\n",
    "\n",
    "jndir_HCHOdiurnal_data = \"/import/GREENING/tzhao/jndata/HCHOdiurnal_data/\"\n",
    "\n",
    "ap_gc = xr.open_dataarray(jndirGC+\"ap_gc.nc\", engine=\"netcdf4\")\n",
    "b_gc = xr.open_dataarray(jndirGC+\"b_gc.nc\", engine=\"netcdf4\")\n",
    "H_b_gc = xr.open_dataarray(jndirGC+\"H_b_gc.nc\", engine=\"netcdf4\")\n",
    "P_geoscf = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3633f22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "P_geoscf = [\n",
    "    [1, 0.0100], [2, 0.0200], [3, 0.0327], [4, 0.0476], [5, 0.0660], [6, 0.0893], [7, 0.1197], [8, 0.1595], [9, 0.2113], [10, 0.2785], \n",
    "    [11, 0.3650], [12, 0.4758], [13, 0.6168], [14, 0.7951], [15, 1.0194], [16, 1.3005], [17, 1.6508], [18, 2.0850], [19, 2.6202], \n",
    "    [20, 3.2764], [21, 4.0766], [22, 5.0468], [23, 6.2168], [24, 7.6198], [25, 9.2929], [26, 11.2769], [27, 13.6434], [28, 16.4571], \n",
    "    [29, 19.7916], [30, 23.7304], [31, 28.3678], [32, 33.8100], [33, 40.1754], [34, 47.6439], [35, 56.3879], [36, 66.6034], \n",
    "    [37, 78.5123], [38, 92.3657], [39, 108.663], [40, 127.837], [41, 150.393], [42, 176.930], [43, 208.152], [44, 244.875], \n",
    "    [45, 288.083], [46, 337.500], [47, 375.000], [48, 412.500], [49, 450.000], [50, 487.500], [51, 525.000], [52, 562.500], \n",
    "    [53, 600.000], [54, 637.500], [55, 675.000], [56, 700.000], [57, 725.000], [58, 750.000], [59, 775.000], [60, 800.000], \n",
    "    [61, 820.000], [62, 835.000], [63, 850.000], [64, 865.000], [65, 880.000], [66, 895.000], [67, 910.000], [68, 925.000], \n",
    "    [69, 940.000], [70, 955.000], [71, 970.000], [72, 985.000]\n",
    "]\n",
    "\n",
    "# Convert to numpy array\n",
    "P_geoscf = np.array(P_geoscf)[36:72,1]\n",
    "\n",
    "H_geoscf = 153.8 * (20+273.2)* (1- (P_geoscf / 1000) ** 0.1902 )   * 1e-3   # km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca2686b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07847e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directory_list(path):\n",
    "    # Get a list of all entries in the directory\n",
    "    entries = os.listdir(path)\n",
    "    # Filter the list to only include directories\n",
    "    directories = [e for e in entries if os.path.isdir(os.path.join(path, e))]\n",
    "    return directories\n",
    "\n",
    "\n",
    "def count_txt_endlines(filename):\n",
    "    import codecs\n",
    "    with codecs.open(filename, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        lines = f.readlines()\n",
    "        num_lines = len(lines)\n",
    "    return num_lines\n",
    "\n",
    "\n",
    "def extract_values_from_line(filename, line_number):\n",
    "    import re\n",
    "\n",
    "    # Open the file in read-only mode\n",
    "    with open(filename, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        # Read the specified line from the file\n",
    "        line = file.readlines()[line_number - 1]\n",
    "\n",
    "        # Use a regular expression to extract the values\n",
    "        # The regular expression looks for a pattern of one or more non-whitespace characters, followed by \": \", followed by one or more non-whitespace characters\n",
    "        match = re.search(r'(.*): (.*)', line)\n",
    "        if match:\n",
    "            # If the pattern is found, return the two groups of characters\n",
    "            # If the second group is empty, set the value to \"None\"\n",
    "            key = match.group(1)\n",
    "            value = match.group(2)\n",
    "            if value:\n",
    "                return key, value\n",
    "            else:\n",
    "                return key, \"None\"\n",
    "        else:\n",
    "            # If the pattern is not found, return None\n",
    "            return None\n",
    "\n",
    "def calculate_diurnal_cycle(data, lat, lon, group=None):\n",
    "    import timezonefinder\n",
    "    import pytz\n",
    "    \n",
    "    # Determine the time zone based on the latitude and longitude\n",
    "    tf = timezonefinder.TimezoneFinder()\n",
    "    tz = pytz.timezone(tf.certain_timezone_at(lat=lat, lng=lon))\n",
    "\n",
    "    hour_LT_list = list( data['time'].to_index().tz_localize(pytz.utc).tz_convert(tz).hour )\n",
    "    hour_LT_da = xr.DataArray( hour_LT_list, coords=[data.time], dims=['time'])\n",
    "    diurnal_cycle = data.groupby(hour_LT_da).mean(dim='time')\n",
    "    if group:\n",
    "        del(diurnal_cycle)\n",
    "        diurnal_cycle = data.groupby(hour_LT_da)\n",
    "    return diurnal_cycle\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_diurnal_cycle_GC(data, lat, lon):\n",
    "    \n",
    "    import timezonefinder\n",
    "    import pytz\n",
    "\n",
    "    # Determine the time zone based on the latitude and longitude\n",
    "    tf = timezonefinder.TimezoneFinder()\n",
    "    tz = pytz.timezone(tf.certain_timezone_at(lat=lat, lng=lon))\n",
    "\n",
    "\n",
    "    \n",
    "#     yearlist = np.unique( data.time.dt.year )\n",
    "    yearlist = np.array([2021,2022,2023])\n",
    "    yearnum = yearlist.shape[0]\n",
    "    \n",
    "    # Create datetime array for 2021\n",
    "    start_date_2021 = '2021-05-01 00:00:00'\n",
    "    end_date_2021 = '2021-08-31 00:00:00'\n",
    "    datetime_array_2021 = pd.date_range(start=start_date_2021, end=end_date_2021, freq='D')\n",
    "    # Create datetime array for 2022\n",
    "    start_date_2022 = '2022-05-01 00:00:00'\n",
    "    end_date_2022 = '2022-08-31 00:00:00'\n",
    "    datetime_array_2022 = pd.date_range(start=start_date_2022, end=end_date_2022, freq='D')\n",
    "    # Create datetime array for 2023\n",
    "    start_date_2023 = '2023-05-01 00:00:00'\n",
    "    end_date_2023 = '2023-08-31 00:00:00'\n",
    "    datetime_array_2023 = pd.date_range(start=start_date_2023, end=end_date_2023, freq='D')\n",
    "    # Combine the two datetime arrays\n",
    "    datetime_array_combined = datetime_array_2021.union(datetime_array_2022)\n",
    "    datetime_array_combined = datetime_array_combined.union(datetime_array_2023).copy()\n",
    "    \n",
    "    \n",
    "    \n",
    "    if len(data.shape)==1:\n",
    "        data_hour_day = np.full((24, 123*yearnum), np.nan)\n",
    "\n",
    "        iyear = 0\n",
    "        for YYYY in yearlist:\n",
    "\n",
    "            data_YYYY = data[ data.time.dt.year==YYYY ]\n",
    "            data_YYYY = data_YYYY.where(np.abs(data_YYYY)<1e50 )\n",
    "            data_YYYY = data_YYYY.resample(time='1H').mean(dim='time').copy()\n",
    "\n",
    "            hour_LT_list = list( data_YYYY['time'].to_index().tz_localize(pytz.utc).tz_convert(tz).hour )\n",
    "            hour_LT_da = xr.DataArray( hour_LT_list, coords=[data_YYYY.time], dims=['time'])\n",
    "            day_LT_list = list( data_YYYY['time'].to_index().tz_localize(pytz.utc).tz_convert(tz).dayofyear )\n",
    "            day_LT_da = xr.DataArray( day_LT_list, coords=[data_YYYY.time], dims=['time'])\n",
    "            data_YYYY['hour_local'] = hour_LT_da\n",
    "            data_YYYY['day_local'] = day_LT_da\n",
    "\n",
    "            # # Group by date and local hour, then aggregate (mean, sum, etc., based on your data)\n",
    "            grouped = data_YYYY.groupby(day_LT_da)\n",
    "\n",
    "            for iday in range(121,244):\n",
    "                try:\n",
    "                    hour_local = grouped[iday].hour_local.values\n",
    "                    data_hour_day[:,iday-121+123*iyear][hour_local] = grouped[iday]\n",
    "                except:\n",
    "                    print('missed 1 day')\n",
    "                    continue\n",
    "\n",
    "            iyear+=1\n",
    "\n",
    "    if len(data.shape)==2:\n",
    "        data_hour_day = np.full((24, 123*yearnum, data.shape[1]), np.nan)\n",
    "\n",
    "        iyear = 0\n",
    "        for YYYY in yearlist:\n",
    "\n",
    "            data_YYYY = data[ data.time.dt.year==YYYY ]\n",
    "            data_YYYY = data_YYYY.where( np.abs(data_YYYY)<1e50 )\n",
    "            data_YYYY = data_YYYY.resample(time='1H').mean(dim='time').copy()\n",
    "\n",
    "            hour_LT_list = list( data_YYYY['time'].to_index().tz_localize(pytz.utc).tz_convert(tz).hour )\n",
    "            hour_LT_da = xr.DataArray( hour_LT_list, coords=[data_YYYY.time], dims=['time'])\n",
    "            day_LT_list = list( data_YYYY['time'].to_index().tz_localize(pytz.utc).tz_convert(tz).dayofyear )\n",
    "            day_LT_da = xr.DataArray( day_LT_list, coords=[data_YYYY.time], dims=['time'])\n",
    "            data_YYYY['hour_local'] = hour_LT_da\n",
    "            data_YYYY['day_local'] = day_LT_da\n",
    "\n",
    "            # # Group by date and local hour, then aggregate (mean, sum, etc., based on your data)\n",
    "            grouped = data_YYYY.groupby(day_LT_da)\n",
    "\n",
    "            for iday in range(121,244):\n",
    "                try:\n",
    "                    hour_local = grouped[iday].hour_local.values\n",
    "                    data_hour_day[:,iday-121+123*iyear,:][hour_local] = grouped[iday]\n",
    "                except:\n",
    "                    print('missed 1 day')\n",
    "                    continue\n",
    "\n",
    "            iyear+=1\n",
    "\n",
    "\n",
    "    # Set rows with less than 10 non-NaN values to NaN\n",
    "#     criteria_hours = 7    # if for a day have < 10 non-nan values, abandon that day\n",
    "#     data_hour_day[ :, np.sum(~np.isnan(data_hour_day), axis=0) <= criteria_hours] = np.nan\n",
    "\n",
    "    # Calculate the mean and standard deviation for each hour\n",
    "    hourly_means = np.nanmedian(data_hour_day, axis=1)\n",
    "    hourly_stds = np.nanstd(data_hour_day, axis=1)\n",
    "    data_points_count = np.sum(~np.isnan(data_hour_day), axis=1)\n",
    "\n",
    "    if len(data.shape)==1:\n",
    "        diurnal_means = xr.DataArray( hourly_means, coords=[np.arange(0,24)], dims=['hour_local'])\n",
    "        diurnal_stds = xr.DataArray( hourly_stds, coords=[np.arange(0,24)], dims=['hour_local'])\n",
    "        diurnal_amounts = xr.DataArray( data_points_count, coords=[np.arange(0,24)], dims=['hour_local'])\n",
    "        diurnal_arrays = xr.DataArray( data_hour_day, coords=[ np.arange(0,24), datetime_array_combined ], dims=['hour_local','day'])\n",
    "    if len(data.shape)==2:\n",
    "        if data.shape[1]==12:\n",
    "            diurnal_means = xr.DataArray( hourly_means, coords=[np.arange(0,24), PGN_fuh_HCHOdVCDprofile_2021to2023_warm.layer_center_height_bins.values], dims=['hour_local','layer_center_height_bins'])\n",
    "            diurnal_stds = xr.DataArray( hourly_stds, coords=[np.arange(0,24), PGN_fuh_HCHOdVCDprofile_2021to2023_warm.layer_center_height_bins.values], dims=['hour_local','layer_center_height_bins'])\n",
    "            diurnal_amounts = xr.DataArray( data_points_count, coords=[np.arange(0,24), PGN_fuh_HCHOdVCDprofile_2021to2023_warm.layer_center_height_bins.values], dims=['hour_local','layer_center_height_bins'])\n",
    "            diurnal_arrays = xr.DataArray( data_hour_day, coords=[np.arange(0,24), datetime_array_combined , PGN_fuh_HCHOdVCDprofile_2021to2023_warm.layer_center_height_bins.values], dims=['hour_local','day','layer_center_height_bins'])\n",
    "        elif data.shape[1]==36:\n",
    "            diurnal_means = xr.DataArray( hourly_means, coords=[np.arange(0,24), GEOSCF_HCHO_v36_2021_warm.lev], dims=['hour_local','lev'])\n",
    "            diurnal_stds = xr.DataArray( hourly_stds, coords=[np.arange(0,24), GEOSCF_HCHO_v36_2021_warm.lev], dims=['hour_local','lev'])\n",
    "            diurnal_amounts = xr.DataArray( data_points_count, coords=[np.arange(0,24), GEOSCF_HCHO_v36_2021_warm.lev], dims=['hour_local','lev'])\n",
    "            diurnal_arrays = xr.DataArray( data_hour_day, coords=[np.arange(0,24), datetime_array_combined , GEOSCF_HCHO_v36_2021_warm.lev], dims=['hour_local','day','lev'])\n",
    "\n",
    "    return diurnal_means, diurnal_stds, diurnal_amounts, diurnal_arrays\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_diurnal_cycle_GC_cold(data, lat, lon):\n",
    "    \n",
    "    import timezonefinder\n",
    "    import pytz\n",
    "\n",
    "    # Determine the time zone based on the latitude and longitude\n",
    "    tf = timezonefinder.TimezoneFinder()\n",
    "    tz = pytz.timezone(tf.certain_timezone_at(lat=lat, lng=lon))\n",
    "\n",
    "\n",
    "    \n",
    "#     yearlist = np.unique( data.time.dt.year )\n",
    "    yearlist = np.array([2021,2022,2023])\n",
    "    yearnum = yearlist.shape[0]\n",
    "    \n",
    "    # Create datetime array for 2021\n",
    "    start_date_2021 = '2021-01-01 00:00:00'\n",
    "    end_date_2021 = '2021-02-28 00:00:00'\n",
    "    datetime_array_2021 = pd.date_range(start=start_date_2021, end=end_date_2021, freq='D')\n",
    "    start_date_2021_part2 = '2021-11-01 00:00:00'\n",
    "    end_date_2021_part2 = '2021-12-31 00:00:00'\n",
    "    datetime_array_2021_part2 = pd.date_range(start=start_date_2021_part2, end=end_date_2021_part2, freq='D')\n",
    "    # Combine the two datetime arrays\n",
    "    datetime_array_2021_combined = datetime_array_2021.union(datetime_array_2021_part2)\n",
    "    \n",
    "    \n",
    "    # Create datetime array for 2022\n",
    "    start_date_2022 = '2022-01-01 00:00:00'\n",
    "    end_date_2022 = '2022-02-28 00:00:00'\n",
    "    datetime_array_2022 = pd.date_range(start=start_date_2022, end=end_date_2022, freq='D')\n",
    "    start_date_2022_part2 = '2022-11-01 00:00:00'\n",
    "    end_date_2022_part2 = '2022-12-31 00:00:00'\n",
    "    datetime_array_2022_part2 = pd.date_range(start=start_date_2022_part2, end=end_date_2022_part2, freq='D')\n",
    "    # Combine the two datetime arrays\n",
    "    datetime_array_2022_combined = datetime_array_2022.union(datetime_array_2022_part2)\n",
    "    \n",
    "    # Create datetime array for 2023\n",
    "    start_date_2023 = '2023-01-01 00:00:00'\n",
    "    end_date_2023 = '2023-02-28 00:00:00'\n",
    "    datetime_array_2023 = pd.date_range(start=start_date_2023, end=end_date_2023, freq='D')\n",
    "    start_date_2023_part2 = '2023-11-01 00:00:00'\n",
    "    end_date_2023_part2 = '2023-12-31 00:00:00'\n",
    "    datetime_array_2023_part2 = pd.date_range(start=start_date_2023_part2, end=end_date_2023_part2, freq='D')\n",
    "    # Combine the two datetime arrays\n",
    "    datetime_array_2023_combined = datetime_array_2023.union(datetime_array_2023_part2)\n",
    "\n",
    "    \n",
    "    # Combine the two datetime arrays\n",
    "    datetime_array_combined = datetime_array_2021_combined.union(datetime_array_2022_combined)\n",
    "    datetime_array_combined = datetime_array_combined.union(datetime_array_2023_combined).copy()\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    if len(data.shape)==1:\n",
    "        data_hour_day = np.full((24, 120*yearnum), np.nan)\n",
    "\n",
    "        iyear = 0\n",
    "        for YYYY in yearlist:\n",
    "            \n",
    "            # Generate date ranges for January 1st to February 28th and November 1st to December 31st\n",
    "            jan_feb_dates = pd.date_range(start=f'{YYYY}-01-01', end=f'{YYYY}-02-28')\n",
    "            nov_dec_dates = pd.date_range(start=f'{YYYY}-11-01', end=f'{YYYY}-12-31')\n",
    "            # Extract the day of the year for each date\n",
    "            jan_feb_dayofyear = jan_feb_dates.dayofyear\n",
    "            nov_dec_dayofyear = nov_dec_dates.dayofyear\n",
    "            # Combine the day of the year arrays\n",
    "            dayofyear_list = np.concatenate([jan_feb_dayofyear, nov_dec_dayofyear])\n",
    "\n",
    "            # --------  start --------------------------------\n",
    "\n",
    "            data_YYYY = data[ data.time.dt.year==YYYY ]\n",
    "            data_YYYY = data_YYYY.where(np.abs(data_YYYY)<1e50 )\n",
    "            data_YYYY = data_YYYY.resample(time='1H').mean(dim='time').copy()\n",
    "\n",
    "            hour_LT_list = list( data_YYYY['time'].to_index().tz_localize(pytz.utc).tz_convert(tz).hour )\n",
    "            hour_LT_da = xr.DataArray( hour_LT_list, coords=[data_YYYY.time], dims=['time'])\n",
    "            day_LT_list = list( data_YYYY['time'].to_index().tz_localize(pytz.utc).tz_convert(tz).dayofyear )\n",
    "            day_LT_da = xr.DataArray( day_LT_list, coords=[data_YYYY.time], dims=['time'])\n",
    "            data_YYYY['hour_local'] = hour_LT_da\n",
    "            data_YYYY['day_local'] = day_LT_da\n",
    "\n",
    "            # # Group by date and local hour, then aggregate (mean, sum, etc., based on your data)\n",
    "            grouped = data_YYYY.groupby(day_LT_da)\n",
    "\n",
    "            for iday in dayofyear_list:\n",
    "                try:\n",
    "                    hour_local = grouped[iday].hour_local.values\n",
    "                    data_hour_day[:, np.where(dayofyear_list==iday)[0][0] + 120*iyear ][hour_local] = grouped[iday]\n",
    "                except:\n",
    "                    continue\n",
    "            iyear+=1\n",
    "\n",
    "    if len(data.shape)==2:\n",
    "        data_hour_day = np.full((24, 120*yearnum, data.shape[1]), np.nan)\n",
    "\n",
    "        iyear = 0\n",
    "        for YYYY in yearlist:\n",
    "            \n",
    "            # Generate date ranges for January 1st to February 28th and November 1st to December 31st\n",
    "            jan_feb_dates = pd.date_range(start=f'{YYYY}-01-01', end=f'{YYYY}-02-28')\n",
    "            nov_dec_dates = pd.date_range(start=f'{YYYY}-11-01', end=f'{YYYY}-12-31')\n",
    "            # Extract the day of the year for each date\n",
    "            jan_feb_dayofyear = jan_feb_dates.dayofyear\n",
    "            nov_dec_dayofyear = nov_dec_dates.dayofyear\n",
    "            # Combine the day of the year arrays\n",
    "            dayofyear_list = np.concatenate([jan_feb_dayofyear, nov_dec_dayofyear])\n",
    "\n",
    "            # --------  start --------------------------------\n",
    "\n",
    "            data_YYYY = data[ data.time.dt.year==YYYY ]\n",
    "            data_YYYY = data_YYYY.where( np.abs(data_YYYY)<1e50 )\n",
    "            data_YYYY = data_YYYY.resample(time='1H').mean(dim='time').copy()\n",
    "\n",
    "            hour_LT_list = list( data_YYYY['time'].to_index().tz_localize(pytz.utc).tz_convert(tz).hour )\n",
    "            hour_LT_da = xr.DataArray( hour_LT_list, coords=[data_YYYY.time], dims=['time'])\n",
    "            day_LT_list = list( data_YYYY['time'].to_index().tz_localize(pytz.utc).tz_convert(tz).dayofyear )\n",
    "            day_LT_da = xr.DataArray( day_LT_list, coords=[data_YYYY.time], dims=['time'])\n",
    "            data_YYYY['hour_local'] = hour_LT_da\n",
    "            data_YYYY['day_local'] = day_LT_da\n",
    "\n",
    "            # # Group by date and local hour, then aggregate (mean, sum, etc., based on your data)\n",
    "            grouped = data_YYYY.groupby(day_LT_da)\n",
    "\n",
    "            for iday in dayofyear_list:\n",
    "                try:\n",
    "                    hour_local = grouped[iday].hour_local.values\n",
    "                    data_hour_day[:, np.where(dayofyear_list==iday)[0][0] + 120*iyear,:][hour_local] = grouped[iday]\n",
    "                except:\n",
    "                    continue\n",
    "            iyear+=1\n",
    "\n",
    "\n",
    "    # Set rows with less than 10 non-NaN values to NaN\n",
    "#     criteria_hours = 7    # if for a day have < 10 non-nan values, abandon that day\n",
    "#     data_hour_day[ :, np.sum(~np.isnan(data_hour_day), axis=0) <= criteria_hours] = np.nan\n",
    "    \n",
    "    # Calculate the mean and standard deviation for each hour\n",
    "    hourly_means = np.nanmedian(data_hour_day, axis=1)\n",
    "    hourly_stds = np.nanstd(data_hour_day, axis=1)\n",
    "    data_points_count = np.sum(~np.isnan(data_hour_day), axis=1)\n",
    "\n",
    "    if len(data.shape)==1:\n",
    "        diurnal_means = xr.DataArray( hourly_means, coords=[np.arange(0,24)], dims=['hour_local'])\n",
    "        diurnal_stds = xr.DataArray( hourly_stds, coords=[np.arange(0,24)], dims=['hour_local'])\n",
    "        diurnal_amounts = xr.DataArray( data_points_count, coords=[np.arange(0,24)], dims=['hour_local'])\n",
    "        diurnal_arrays = xr.DataArray( data_hour_day, coords=[ np.arange(0,24), datetime_array_combined ], dims=['hour_local','day'])\n",
    "    if len(data.shape)==2:\n",
    "        if data.shape[1]==12:\n",
    "            diurnal_means = xr.DataArray( hourly_means, coords=[np.arange(0,24), PGN_fuh_HCHOdVCDprofile_2021to2023_warm.layer_center_height_bins.values], dims=['hour_local','layer_center_height_bins'])\n",
    "            diurnal_stds = xr.DataArray( hourly_stds, coords=[np.arange(0,24), PGN_fuh_HCHOdVCDprofile_2021to2023_warm.layer_center_height_bins.values], dims=['hour_local','layer_center_height_bins'])\n",
    "            diurnal_amounts = xr.DataArray( data_points_count, coords=[np.arange(0,24), PGN_fuh_HCHOdVCDprofile_2021to2023_warm.layer_center_height_bins.values], dims=['hour_local','layer_center_height_bins'])\n",
    "            diurnal_arrays = xr.DataArray( data_hour_day, coords=[np.arange(0,24), datetime_array_combined , PGN_fuh_HCHOdVCDprofile_2021to2023_warm.layer_center_height_bins.values], dims=['hour_local','day','layer_center_height_bins'])\n",
    "        elif data.shape[1]==36:\n",
    "            diurnal_means = xr.DataArray( hourly_means, coords=[np.arange(0,24), GEOSCF_HCHO_v36_2021_warm.lev], dims=['hour_local','lev'])\n",
    "            diurnal_stds = xr.DataArray( hourly_stds, coords=[np.arange(0,24), GEOSCF_HCHO_v36_2021_warm.lev], dims=['hour_local','lev'])\n",
    "            diurnal_amounts = xr.DataArray( data_points_count, coords=[np.arange(0,24), GEOSCF_HCHO_v36_2021_warm.lev], dims=['hour_local','lev'])\n",
    "            diurnal_arrays = xr.DataArray( data_hour_day, coords=[np.arange(0,24), datetime_array_combined , GEOSCF_HCHO_v36_2021_warm.lev], dims=['hour_local','day','lev'])\n",
    "\n",
    "    return diurnal_means, diurnal_stds, diurnal_amounts, diurnal_arrays\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def find_index_in_strlist(input_str, my_list):\n",
    "    for i in range(len(my_list)):\n",
    "        if input_str in my_list[i]:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def sep_sitename_ID(sitename_ID):\n",
    "    \n",
    "    import re\n",
    "    # Your input string\n",
    "    input_string = sitename_ID\n",
    "    # Define a regular expression pattern to match the sitename and number\n",
    "    pattern = r'^(.*?)_P(\\d+)$'\n",
    "    # Use re.search to find the match in the input string\n",
    "    match = re.search(pattern, input_string)\n",
    "    \n",
    "    sitename = match.group(1)\n",
    "    YY = match.group(2)\n",
    "    \n",
    "    return sitename, YY\n",
    "\n",
    "def lookup_info_fush(info_list, short_sitename):\n",
    "    for info in info_list:\n",
    "        if short_sitename in info[2]:\n",
    "            return info\n",
    "    return None  # Return None if the short sitename is not found in any sub-list\n",
    "\n",
    "\n",
    "def ez_cal_diurnal_GC( ds , sitenamelist):\n",
    "    sitename_datasets = {}\n",
    "    for sitename in sitenamelist:\n",
    "        lat = float( lookup_info_fush( info_fush_union, sitename )[0] )\n",
    "        lon = float( lookup_info_fush( info_fush_union, sitename )[1] )\n",
    "        ds_diurnal = calculate_diurnal_cycle_GC( ds[sitename], lat, lon )\n",
    "\n",
    "        # Store the selected data in the dictionary using the sitename as the key\n",
    "        sitename_datasets[sitename] = ds_diurnal\n",
    "        \n",
    "    # Merge all the datasets into a single xarray dataset\n",
    "    merged_dataset = xr.Dataset(sitename_datasets)\n",
    "    return merged_dataset\n",
    "\n",
    "\n",
    "def filter_sites_in_region(info_array, region):\n",
    "    def is_within_region(lat, lon, region):\n",
    "        if region =='Global':\n",
    "            # Define the bounds of North America\n",
    "            region_bounds = {\n",
    "                'min_lat': -90.0,    # Minimum latitude\n",
    "                'max_lat': 90.0,    # Maximum latitude\n",
    "                'min_lon': -180.0,  # Minimum longitude\n",
    "                'max_lon': 180.0    # Maximum longitude\n",
    "            }\n",
    "        if region =='NA':\n",
    "            # Define the bounds of North America\n",
    "            region_bounds = {\n",
    "                'min_lat': 10.0,    # Minimum latitude\n",
    "                'max_lat': 80.0,    # Maximum latitude\n",
    "                'min_lon': -180.0,  # Minimum longitude\n",
    "                'max_lon': -35.0    # Maximum longitude\n",
    "#                 'min_lat': 10.0,    # Minimum latitude\n",
    "#                 'max_lat': 50.0,    # Maximum latitude\n",
    "#                 'min_lon': -125.0,  # Minimum longitude\n",
    "#                 'max_lon': -70.0    # Maximum longitude\n",
    "            }\n",
    "        elif region =='EUS':\n",
    "            # Define the bounds of North America\n",
    "            region_bounds = {\n",
    "                'min_lat': 30.0,    # Minimum latitude\n",
    "                'max_lat': 47.0,    # Maximum latitude\n",
    "                'min_lon': -78.0,  # Minimum longitude\n",
    "                'max_lon': -60.0    # Maximum longitude\n",
    "            }\n",
    "        elif region =='SEUS':\n",
    "            # Define the bounds of North America\n",
    "            region_bounds = {\n",
    "                'min_lat': 25.0,    # Minimum latitude\n",
    "                'max_lat': 36,    # Maximum latitude\n",
    "                'min_lon': -90.0,  # Minimum longitude\n",
    "                'max_lon': -75.0    # Maximum longitude\n",
    "            }\n",
    "        elif region =='NEUS':\n",
    "            # Define the bounds of North America\n",
    "            region_bounds = {\n",
    "                'min_lat': 40.0,    # Minimum latitude\n",
    "                'max_lat': 50,    # Maximum latitude\n",
    "                'min_lon': -85.0,  # Minimum longitude\n",
    "                'max_lon': -75.0    # Maximum longitude\n",
    "            }\n",
    "        elif region =='SUS':\n",
    "            # Define the bounds of North America\n",
    "            region_bounds = {\n",
    "                'min_lat': 20.0,    # Minimum latitude\n",
    "#                 'min_lat': 10.0,    # Minimum latitude\n",
    "                'max_lat': 30,    # Maximum latitude\n",
    "                'min_lon': -110.0,  # Minimum longitude\n",
    "                'max_lon': -90.0    # Maximum longitude\n",
    "            }\n",
    "        elif region =='MWUS':\n",
    "            # Define the bounds of North America\n",
    "            region_bounds = {\n",
    "                'min_lat': 20.0,    # Minimum latitude\n",
    "                'max_lat': 50,    # Maximum latitude\n",
    "                'min_lon': -125.0,  # Minimum longitude\n",
    "                'max_lon': -105.0    # Maximum longitude\n",
    "            }\n",
    "            \n",
    "            \n",
    "        elif region == 'EU':\n",
    "            # Define the bounds of North America\n",
    "            region_bounds = {\n",
    "                'min_lat': 25.0,    # Minimum latitude\n",
    "                'max_lat': 85.0,    # Maximum latitude\n",
    "                'min_lon': -15.0,   # Minimum longitude\n",
    "                'max_lon': 50.0     # Maximum longitude\n",
    "#                 'min_lat': 35.0,    # Minimum latitude\n",
    "#                 'max_lat': 71.0,    # Maximum latitude\n",
    "#                 'min_lon': -35.0,   # Minimum longitude\n",
    "#                 'max_lon': 42.0     # Maximum longitude\n",
    "                \n",
    "            }\n",
    "        elif region == 'AS':\n",
    "            # Define the bounds of North America\n",
    "            region_bounds = {\n",
    "                'min_lat': -10.0,    # Minimum latitude\n",
    "                'max_lat': 60.0,    # Maximum latitude\n",
    "                'min_lon': 90.0,  # Minimum longitude\n",
    "                'max_lon': 180.0    # Maximum longitude\n",
    "            }\n",
    "  \n",
    "        # Check if the coordinates are within the North America bounds\n",
    "        if (region_bounds['min_lat'] <= lat <= region_bounds['max_lat'] and\n",
    "            region_bounds['min_lon'] <= lon <= region_bounds['max_lon']):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    # Create a mask to filter sites within ???\n",
    "    mask = [is_within_region(float(lat), float(lon), region) for lat, lon, _, _, _ in info_array]\n",
    "\n",
    "    # Use the mask to select sites within North America\n",
    "    region_sites = info_array[mask]\n",
    "\n",
    "    return region_sites\n",
    "\n",
    "\n",
    "# Define a custom function for calculating the mean\n",
    "def custom_mean(arr, axis=None):\n",
    "\n",
    "    # Calculate the fraction of NaNs in each variable\n",
    "    nan_fraction = arr.isnull().mean(dim='group')\n",
    "\n",
    "    # Filter out variables where more than 50% of values are NaN\n",
    "    filtered_data_array = arr.sel(variable=nan_fraction <= 0.7)\n",
    "\n",
    "    # Where the count of non-NaN values is greater than the threshold, calculate the mean\n",
    "    # Otherwise, return NaN\n",
    "    return filtered_data_array.mean(dim='variable')\n",
    "\n",
    "\n",
    "def concat_dataset_2021_2022(ds1, ds2):\n",
    "    # Step 1: Align the datasets\n",
    "    # Find the union of all variable names\n",
    "    all_vars = set(ds1.data_vars).union(set(ds2.data_vars))\n",
    "\n",
    "    # Add missing variables as NaNs\n",
    "    for var in all_vars:\n",
    "        if var not in ds1:\n",
    "            ds1[var] = xr.full_like(ds1[list(ds1.data_vars)[0]], np.nan)\n",
    "        if var not in ds2:\n",
    "            ds2[var] = xr.full_like(ds2[list(ds2.data_vars)[0]], np.nan)\n",
    "\n",
    "    # Step 3: Concatenate the datasets\n",
    "    ds12 = xr.concat([ds1, ds2], dim='time')\n",
    "    return ds12\n",
    "\n",
    "\n",
    "def relative_amplitude(arr, axis=1):\n",
    "    # Calculate max, min, and mean along axis=1\n",
    "    max_values = np.nanmax(arr, axis=axis)\n",
    "    min_values = np.nanmin(arr, axis=axis)\n",
    "    mean_values = np.nanmean(arr[12:15], axis=axis)\n",
    "\n",
    "    # Calculate relative amplitude\n",
    "    relative_amplitudes = (max_values - min_values) / mean_values\n",
    "#     relative_amplitudes = (max_values - min_values)\n",
    "#     relative_amplitudes = (arr)\n",
    "    return relative_amplitudes\n",
    "\n",
    "def local_noon(arr, axis=1):\n",
    "    \n",
    "    mean_values = np.nanmean(arr[12:15,:], axis=axis)\n",
    "    return mean_values\n",
    "\n",
    "\n",
    "def lighten_color(color, amount=0.5):\n",
    "    import matplotlib.colors as mcolors\n",
    "    \"\"\"\n",
    "    Lightens the given color by mixing it with white.\n",
    "\n",
    "    Args:\n",
    "    color (str): The name of the original color.\n",
    "    amount (float): A value between 0 and 1, where 0 is the original color and 1 is white.\n",
    "\n",
    "    Returns:\n",
    "    tuple: The lightened color in RGB.\n",
    "    \"\"\"\n",
    "    # Convert the color name to RGB\n",
    "    try:\n",
    "        base_color = mcolors.to_rgb(color)\n",
    "    except ValueError:\n",
    "        print(f\"Color '{color}' is not recognized. Returning white.\")\n",
    "        return (1, 1, 1)\n",
    "\n",
    "    # Mix the color with white\n",
    "    white = np.array([1, 1, 1])\n",
    "    original = np.array(base_color)\n",
    "    new_color = original + (white - original) * amount\n",
    "    return tuple(new_color)\n",
    "\n",
    "\n",
    "def desaturate_color(color_name, desaturation_factor=0.5):\n",
    "    \"\"\"\n",
    "    Reduce the saturation of a given color name.\n",
    "    \n",
    "    :param color_name: The name of the color (e.g., 'red', 'blue')\n",
    "    :param desaturation_factor: Factor to reduce saturation (between 0 and 1)\n",
    "    :return: The color with reduced saturation (in RGB format)\n",
    "    \"\"\"\n",
    "    import matplotlib.colors as mcolors\n",
    "    import colorsys\n",
    "    \n",
    "    # Convert the color name to RGB\n",
    "    rgb = mcolors.to_rgb(color_name)\n",
    "    \n",
    "    # Convert RGB to HSL\n",
    "    h, l, s = colorsys.rgb_to_hls(*rgb)\n",
    "    \n",
    "    # Reduce saturation\n",
    "    s *= desaturation_factor\n",
    "    \n",
    "    # Convert HSL back to RGB\n",
    "    desaturated_rgb = colorsys.hls_to_rgb(h, l, s)\n",
    "    \n",
    "    return desaturated_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d5e93b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_diurnal_cycle_GC_10min(data, lat, lon):\n",
    "    \n",
    "    import timezonefinder\n",
    "    import pytz\n",
    "\n",
    "    # Determine the time zone based on the latitude and longitude\n",
    "    tf = timezonefinder.TimezoneFinder()\n",
    "    tz = pytz.timezone(tf.certain_timezone_at(lat=lat, lng=lon))\n",
    "\n",
    "\n",
    "\n",
    "#     yearlist = np.unique( data.time.dt.year )\n",
    "    yearlist = np.array([2021,2022,2023])\n",
    "    yearnum = yearlist.shape[0]\n",
    "    \n",
    "    # Create datetime array for 2021\n",
    "    start_date_2021 = '2021-05-01 00:00:00'\n",
    "    end_date_2021 = '2021-08-31 00:00:00'\n",
    "    datetime_array_2021 = pd.date_range(start=start_date_2021, end=end_date_2021, freq='D')\n",
    "    # Create datetime array for 2022\n",
    "    start_date_2022 = '2022-05-01 00:00:00'\n",
    "    end_date_2022 = '2022-08-31 00:00:00'\n",
    "    datetime_array_2022 = pd.date_range(start=start_date_2022, end=end_date_2022, freq='D')\n",
    "    # Create datetime array for 2023\n",
    "    start_date_2023 = '2023-05-01 00:00:00'\n",
    "    end_date_2023 = '2023-08-31 00:00:00'\n",
    "    datetime_array_2023 = pd.date_range(start=start_date_2023, end=end_date_2023, freq='D')\n",
    "    # Combine the two datetime arrays\n",
    "    datetime_array_combined = datetime_array_2021.union(datetime_array_2022)\n",
    "    datetime_array_combined = datetime_array_combined.union(datetime_array_2023).copy()\n",
    "\n",
    "\n",
    "\n",
    "    if len(data.shape)==1:\n",
    "        data_hour_day = np.full((144, 123*yearnum), np.nan)\n",
    "\n",
    "        iyear = 0\n",
    "        for YYYY in yearlist:\n",
    "            \n",
    "            # Generate date ranges for January 1st to February 28th and November 1st to December 31st\n",
    "            may_aug_dates = pd.date_range(start=f'{YYYY}-05-01', end=f'{YYYY}-08-31')\n",
    "           # Extract the day of the year for each date\n",
    "            dayofyear_list = may_aug_dates.dayofyear\n",
    "\n",
    "\n",
    "            data_YYYY = data[ data.time.dt.year==YYYY ]\n",
    "            data_YYYY = data_YYYY.where(np.abs(data_YYYY)<1e50 )\n",
    "\n",
    "            hour_LT_list = list( data_YYYY['time'].to_index().tz_localize(pytz.utc).tz_convert(tz).hour )\n",
    "            minute_LT_list = list( data_YYYY['time'].to_index().tz_localize(pytz.utc).tz_convert(tz).minute // 10 )\n",
    "            hour_minute_LT_list = [hour+minute/6.0   for hour, minute in zip(hour_LT_list, minute_LT_list)]\n",
    "            hour_minute_LT_da = xr.DataArray(hour_minute_LT_list, coords=[data_YYYY.time], dims=['time'])\n",
    "            day_LT_list = list( data_YYYY['time'].to_index().tz_localize(pytz.utc).tz_convert(tz).dayofyear )\n",
    "            day_LT_da = xr.DataArray( day_LT_list, coords=[data_YYYY.time], dims=['time'])\n",
    "\n",
    "            \n",
    "            data_YYYY['hour_local'] = hour_minute_LT_da\n",
    "            data_YYYY['day_local'] = day_LT_da\n",
    "            \n",
    "\n",
    "        \n",
    "            # # Group by date and local hour, then aggregate (mean, sum, etc., based on your data)\n",
    "            grouped = data_YYYY.groupby(day_LT_da)\n",
    "\n",
    "            for iday in range(121,244):\n",
    "                try:\n",
    "                    hourly_data = grouped[iday]\n",
    "                    if len(hourly_data==144):\n",
    "                        data_hour_day[:, np.where(dayofyear_list==iday)[0][0] + 120*iyear ] = hourly_data\n",
    "                except:\n",
    "                    print('missed 1 day')\n",
    "                    continue\n",
    "\n",
    "            iyear+=1\n",
    "\n",
    "    if len(data.shape)==2:\n",
    "        data_hour_day = np.full((144, 123*yearnum, data.shape[1]), np.nan)\n",
    "\n",
    "        iyear = 0\n",
    "        for YYYY in yearlist:\n",
    "            \n",
    "            # Generate date ranges for January 1st to February 28th and November 1st to December 31st\n",
    "            may_aug_dates = pd.date_range(start=f'{YYYY}-05-01', end=f'{YYYY}-08-31')\n",
    "           # Extract the day of the year for each date\n",
    "            dayofyear_list = may_aug_dates.dayofyear\n",
    "\n",
    "            data_YYYY = data[ data.time.dt.year==YYYY ]\n",
    "            data_YYYY = data_YYYY.where( np.abs(data_YYYY)<1e50 )\n",
    "\n",
    "            hour_LT_list = list( data_YYYY['time'].to_index().tz_localize(pytz.utc).tz_convert(tz).hour )\n",
    "            minute_LT_list = list( data_YYYY['time'].to_index().tz_localize(pytz.utc).tz_convert(tz).minute // 10 )\n",
    "            hour_minute_LT_list = [hour+minute/6.0   for hour, minute in zip(hour_LT_list, minute_LT_list)]\n",
    "            hour_minute_LT_da = xr.DataArray(hour_minute_LT_list, coords=[data_YYYY.time], dims=['time'])\n",
    "            day_LT_list = list( data_YYYY['time'].to_index().tz_localize(pytz.utc).tz_convert(tz).dayofyear )\n",
    "            day_LT_da = xr.DataArray( day_LT_list, coords=[data_YYYY.time], dims=['time'])\n",
    "            \n",
    "            \n",
    "            data_YYYY['hour_local'] = hour_minute_LT_da\n",
    "            data_YYYY['day_local'] = day_LT_da\n",
    "\n",
    "            # # Group by date and local hour, then aggregate (mean, sum, etc., based on your data)\n",
    "            grouped = data_YYYY.groupby(day_LT_da)\n",
    "\n",
    "            for iday in range(121,244):\n",
    "                try:\n",
    "                    hourly_data = grouped[iday]\n",
    "                    if len(hourly_data==144):\n",
    "                        data_hour_day[:, np.where(dayofyear_list==iday)[0][0] + 120*iyear ] = hourly_data\n",
    "                except:\n",
    "                    print('missed 1 day')\n",
    "                    continue\n",
    "\n",
    "            iyear+=1\n",
    "\n",
    "\n",
    "    # Set rows with less than 10 non-NaN values to NaN\n",
    "#     criteria_hours = 7*6    # if for a day have < 10 non-nan values, abandon that day\n",
    "#     data_hour_day[ :, np.sum(~np.isnan(data_hour_day), axis=0) <= criteria_hours] = np.nan\n",
    "\n",
    "    # Calculate the mean and standard deviation for each hour\n",
    "    hourly_means = np.nanmedian(data_hour_day, axis=1)\n",
    "    hourly_stds = np.nanstd(data_hour_day, axis=1)\n",
    "    data_points_count = np.sum(~np.isnan(data_hour_day), axis=1)\n",
    "\n",
    "    if len(data.shape)==1:\n",
    "        diurnal_means = xr.DataArray( hourly_means, coords=[np.arange(0,144)], dims=['hour_local'])\n",
    "        diurnal_stds = xr.DataArray( hourly_stds, coords=[np.arange(0,144)], dims=['hour_local'])\n",
    "        diurnal_amounts = xr.DataArray( data_points_count, coords=[np.arange(0,144)], dims=['hour_local'])\n",
    "        diurnal_arrays = xr.DataArray( data_hour_day, coords=[ np.arange(0,144), datetime_array_combined ], dims=['hour_local','day'])\n",
    "    if len(data.shape)==2:\n",
    "        if data.shape[1]==11:\n",
    "            diurnal_means = xr.DataArray( hourly_means, coords=[np.arange(0,144), PGN_fuh_HCHOdVCDprofile_2021to2023_warm.layer_center_height_bins.values], dims=['hour_local','layer_center_height_bins'])\n",
    "            diurnal_stds = xr.DataArray( hourly_stds, coords=[np.arange(0,144), PGN_fuh_HCHOdVCDprofile_2021to2023_warm.layer_center_height_bins.values], dims=['hour_local','layer_center_height_bins'])\n",
    "            diurnal_amounts = xr.DataArray( data_points_count, coords=[np.arange(0,144), PGN_fuh_HCHOdVCDprofile_2021to2023_warm.layer_center_height_bins.values], dims=['hour_local','layer_center_height_bins'])\n",
    "            diurnal_arrays = xr.DataArray( data_hour_day, coords=[np.arange(0,144), datetime_array_combined , PGN_fuh_HCHOdVCDprofile_2021to2023_warm.layer_center_height_bins.values], dims=['hour_local','day','layer_center_height_bins'])\n",
    "        elif data.shape[1]==36:\n",
    "            diurnal_means = xr.DataArray( hourly_means, coords=[np.arange(0,144), GEOSCF_HCHO_v36_2021_warm.lev], dims=['hour_local','lev'])\n",
    "            diurnal_stds = xr.DataArray( hourly_stds, coords=[np.arange(0,144), GEOSCF_HCHO_v36_2021_warm.lev], dims=['hour_local','lev'])\n",
    "            diurnal_amounts = xr.DataArray( data_points_count, coords=[np.arange(0,144), GEOSCF_HCHO_v36_2021_warm.lev], dims=['hour_local','lev'])\n",
    "            diurnal_arrays = xr.DataArray( data_hour_day, coords=[np.arange(0,144), datetime_array_combined , GEOSCF_HCHO_v36_2021_warm.lev], dims=['hour_local','day','lev'])\n",
    "\n",
    "    return diurnal_means, diurnal_stds, diurnal_amounts, diurnal_arrays\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_diurnal_cycle_GC_cold_10min(data, lat, lon):\n",
    "    \n",
    "    import timezonefinder\n",
    "    import pytz\n",
    "\n",
    "    # Determine the time zone based on the latitude and longitude\n",
    "    tf = timezonefinder.TimezoneFinder()\n",
    "    tz = pytz.timezone(tf.certain_timezone_at(lat=lat, lng=lon))\n",
    "\n",
    "\n",
    "    \n",
    "#     yearlist = np.unique( data.time.dt.year )\n",
    "    yearlist = np.array([2021,2022,2023])\n",
    "    yearnum = yearlist.shape[0]\n",
    "    \n",
    "    # Create datetime array for 2021\n",
    "    start_date_2021 = '2021-01-01 00:00:00'\n",
    "    end_date_2021 = '2021-02-28 00:00:00'\n",
    "    datetime_array_2021 = pd.date_range(start=start_date_2021, end=end_date_2021, freq='D')\n",
    "    start_date_2021_part2 = '2021-11-01 00:00:00'\n",
    "    end_date_2021_part2 = '2021-12-31 00:00:00'\n",
    "    datetime_array_2021_part2 = pd.date_range(start=start_date_2021_part2, end=end_date_2021_part2, freq='D')\n",
    "    # Combine the two datetime arrays\n",
    "    datetime_array_2021_combined = datetime_array_2021.union(datetime_array_2021_part2)\n",
    "    \n",
    "    \n",
    "    # Create datetime array for 2022\n",
    "    start_date_2022 = '2022-01-01 00:00:00'\n",
    "    end_date_2022 = '2022-02-28 00:00:00'\n",
    "    datetime_array_2022 = pd.date_range(start=start_date_2022, end=end_date_2022, freq='D')\n",
    "    start_date_2022_part2 = '2022-11-01 00:00:00'\n",
    "    end_date_2022_part2 = '2022-12-31 00:00:00'\n",
    "    datetime_array_2022_part2 = pd.date_range(start=start_date_2022_part2, end=end_date_2022_part2, freq='D')\n",
    "    # Combine the two datetime arrays\n",
    "    datetime_array_2022_combined = datetime_array_2022.union(datetime_array_2022_part2)\n",
    "    \n",
    "    # Create datetime array for 2023\n",
    "    start_date_2023 = '2023-01-01 00:00:00'\n",
    "    end_date_2023 = '2023-02-28 00:00:00'\n",
    "    datetime_array_2023 = pd.date_range(start=start_date_2023, end=end_date_2023, freq='D')\n",
    "    start_date_2023_part2 = '2023-11-01 00:00:00'\n",
    "    end_date_2023_part2 = '2023-12-31 00:00:00'\n",
    "    datetime_array_2023_part2 = pd.date_range(start=start_date_2023_part2, end=end_date_2023_part2, freq='D')\n",
    "    # Combine the two datetime arrays\n",
    "    datetime_array_2023_combined = datetime_array_2023.union(datetime_array_2023_part2)\n",
    "\n",
    "    \n",
    "    # Combine the two datetime arrays\n",
    "    datetime_array_combined = datetime_array_2021_combined.union(datetime_array_2022_combined)\n",
    "    datetime_array_combined = datetime_array_combined.union(datetime_array_2023_combined).copy()\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    if len(data.shape)==1:\n",
    "        data_hour_day = np.full((144, 120*yearnum), np.nan)\n",
    "\n",
    "        iyear = 0\n",
    "        for YYYY in yearlist:\n",
    "            \n",
    "            # Generate date ranges for January 1st to February 28th and November 1st to December 31st\n",
    "            jan_feb_dates = pd.date_range(start=f'{YYYY}-01-01', end=f'{YYYY}-02-28')\n",
    "            nov_dec_dates = pd.date_range(start=f'{YYYY}-11-01', end=f'{YYYY}-12-31')\n",
    "            # Extract the day of the year for each date\n",
    "            jan_feb_dayofyear = jan_feb_dates.dayofyear\n",
    "            nov_dec_dayofyear = nov_dec_dates.dayofyear\n",
    "            # Combine the day of the year arrays\n",
    "            dayofyear_list = np.concatenate([jan_feb_dayofyear, nov_dec_dayofyear])\n",
    "\n",
    "            # --------  start --------------------------------\n",
    "\n",
    "            data_YYYY = data[ data.time.dt.year==YYYY ]\n",
    "            data_YYYY = data_YYYY.where(np.abs(data_YYYY)<1e50 )\n",
    "\n",
    "            hour_LT_list = list( data_YYYY['time'].to_index().tz_localize(pytz.utc).tz_convert(tz).hour )\n",
    "            minute_LT_list = list( data_YYYY['time'].to_index().tz_localize(pytz.utc).tz_convert(tz).minute // 10 )\n",
    "            hour_minute_LT_list = [hour+minute/6.0   for hour, minute in zip(hour_LT_list, minute_LT_list)]\n",
    "            hour_minute_LT_da = xr.DataArray(hour_minute_LT_list, coords=[data_YYYY.time], dims=['time'])\n",
    "            day_LT_list = list( data_YYYY['time'].to_index().tz_localize(pytz.utc).tz_convert(tz).dayofyear )\n",
    "            day_LT_da = xr.DataArray( day_LT_list, coords=[data_YYYY.time], dims=['time'])\n",
    "\n",
    "            # # Group by date and local hour, then aggregate (mean, sum, etc., based on your data)\n",
    "            grouped = data_YYYY.groupby(day_LT_da)\n",
    "\n",
    "            for iday in dayofyear_list:\n",
    "                try:\n",
    "                    hourly_data = grouped[iday]\n",
    "                    if len(hourly_data==144):\n",
    "                        data_hour_day[:, np.where(dayofyear_list==iday)[0][0] + 120*iyear ] = hourly_data\n",
    "                except:\n",
    "                    continue\n",
    "            iyear+=1\n",
    "\n",
    "    if len(data.shape)==2:\n",
    "        data_hour_day = np.full((144, 120*yearnum, data.shape[1]), np.nan)\n",
    "\n",
    "        iyear = 0\n",
    "        for YYYY in yearlist:\n",
    "            \n",
    "            # Generate date ranges for January 1st to February 28th and November 1st to December 31st\n",
    "            jan_feb_dates = pd.date_range(start=f'{YYYY}-01-01', end=f'{YYYY}-02-28')\n",
    "            nov_dec_dates = pd.date_range(start=f'{YYYY}-11-01', end=f'{YYYY}-12-31')\n",
    "            # Extract the day of the year for each date\n",
    "            jan_feb_dayofyear = jan_feb_dates.dayofyear\n",
    "            nov_dec_dayofyear = nov_dec_dates.dayofyear\n",
    "            # Combine the day of the year arrays\n",
    "            dayofyear_list = np.concatenate([jan_feb_dayofyear, nov_dec_dayofyear])\n",
    "\n",
    "            # --------  start --------------------------------\n",
    "\n",
    "            data_YYYY = data[ data.time.dt.year==YYYY ]\n",
    "            data_YYYY = data_YYYY.where( np.abs(data_YYYY)<1e50 )\n",
    "\n",
    "            hour_LT_list = list( data_YYYY['time'].to_index().tz_localize(pytz.utc).tz_convert(tz).hour )\n",
    "            minute_LT_list = list( data_YYYY['time'].to_index().tz_localize(pytz.utc).tz_convert(tz).minute // 10 )\n",
    "            hour_minute_LT_list = [hour+minute/6.0   for hour, minute in zip(hour_LT_list, minute_LT_list)]\n",
    "            hour_minute_LT_da = xr.DataArray(hour_minute_LT_list, coords=[data_YYYY.time], dims=['time'])\n",
    "            day_LT_list = list( data_YYYY['time'].to_index().tz_localize(pytz.utc).tz_convert(tz).dayofyear )\n",
    "            day_LT_da = xr.DataArray( day_LT_list, coords=[data_YYYY.time], dims=['time'])\n",
    "\n",
    "            # # Group by date and local hour, then aggregate (mean, sum, etc., based on your data)\n",
    "            grouped = data_YYYY.groupby(day_LT_da)\n",
    "\n",
    "            for iday in dayofyear_list:\n",
    "                try:\n",
    "                    hourly_data = grouped[iday]\n",
    "                    if len(hourly_data==144):\n",
    "                        data_hour_day[:, np.where(dayofyear_list==iday)[0][0] + 120*iyear ] = hourly_data\n",
    "                except:\n",
    "                    continue\n",
    "            iyear+=1\n",
    "\n",
    "\n",
    "    # Set rows with less than 10 non-NaN values to NaN\n",
    "#     criteria_hours = 7    # if for a day have < 10 non-nan values, abandon that day\n",
    "#     data_hour_day[ :, np.sum(~np.isnan(data_hour_day), axis=0) <= criteria_hours] = np.nan\n",
    "    \n",
    "    # Calculate the mean and standard deviation for each hour\n",
    "    hourly_means = np.nanmedian(data_hour_day, axis=1)\n",
    "    hourly_stds = np.nanstd(data_hour_day, axis=1)\n",
    "    data_points_count = np.sum(~np.isnan(data_hour_day), axis=1)\n",
    "\n",
    "    if len(data.shape)==1:\n",
    "        diurnal_means = xr.DataArray( hourly_means, coords=[np.arange(0,144)], dims=['hour_local'])\n",
    "        diurnal_stds = xr.DataArray( hourly_stds, coords=[np.arange(0,144)], dims=['hour_local'])\n",
    "        diurnal_amounts = xr.DataArray( data_points_count, coords=[np.arange(0,144)], dims=['hour_local'])\n",
    "        diurnal_arrays = xr.DataArray( data_hour_day, coords=[ np.arange(0,144), datetime_array_combined ], dims=['hour_local','day'])\n",
    "    if len(data.shape)==2:\n",
    "        if data.shape[1]==10:\n",
    "            diurnal_means = xr.DataArray( hourly_means, coords=[np.arange(0,144), PGN_fuh_HCHOdVCDprofile_2021to2023_warm.layer_center_height_bins.values], dims=['hour_local','layer_center_height_bins'])\n",
    "            diurnal_stds = xr.DataArray( hourly_stds, coords=[np.arange(0,144), PGN_fuh_HCHOdVCDprofile_2021to2023_warm.layer_center_height_bins.values], dims=['hour_local','layer_center_height_bins'])\n",
    "            diurnal_amounts = xr.DataArray( data_points_count, coords=[np.arange(0,144), PGN_fuh_HCHOdVCDprofile_2021to2023_warm.layer_center_height_bins.values], dims=['hour_local','layer_center_height_bins'])\n",
    "            diurnal_arrays = xr.DataArray( data_hour_day, coords=[np.arange(0,144), datetime_array_combined , PGN_fuh_HCHOdVCDprofile_2021to2023_warm.layer_center_height_bins.values], dims=['hour_local','day','layer_center_height_bins'])\n",
    "        elif data.shape[1]==36:\n",
    "            diurnal_means = xr.DataArray( hourly_means, coords=[np.arange(0,144), GEOSCF_HCHO_v36_2021_warm.lev], dims=['hour_local','lev'])\n",
    "            diurnal_stds = xr.DataArray( hourly_stds, coords=[np.arange(0,144), GEOSCF_HCHO_v36_2021_warm.lev], dims=['hour_local','lev'])\n",
    "            diurnal_amounts = xr.DataArray( data_points_count, coords=[np.arange(0,144), GEOSCF_HCHO_v36_2021_warm.lev], dims=['hour_local','lev'])\n",
    "            diurnal_arrays = xr.DataArray( data_hour_day, coords=[np.arange(0,144), datetime_array_combined , GEOSCF_HCHO_v36_2021_warm.lev], dims=['hour_local','day','lev'])\n",
    "\n",
    "    return diurnal_means, diurnal_stds, diurnal_amounts, diurnal_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e661d03a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c04703",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5754ebd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b71f82c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f5f23c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "\n",
    "# Initialize Earth Engine\n",
    "ee.Initialize()\n",
    "\n",
    "def classify_area(latitude, longitude, radius_km):\n",
    "    # Define the center point as a geometry\n",
    "    center_point = ee.Geometry.Point(longitude, latitude)\n",
    "\n",
    "    # Define a circular region of interest (ROI) around the center point\n",
    "    roi = center_point.buffer(radius_km * 1000)  # Convert radius from km to meters\n",
    "\n",
    "    # Load the land cover dataset (Copernicus Global Land Cover)\n",
    "    land_cover = ee.Image('COPERNICUS/Landcover/100m/Proba-V-C3/Global/2019')\\\n",
    "        .select('urban-coverfraction')  # Select the 'urban-coverfraction' band\n",
    "\n",
    "    # Clip the land cover dataset to the ROI\n",
    "    clipped_land_cover = land_cover.clip(roi)\n",
    "\n",
    "    # Get the mean value of the 'urban-coverfraction' band within the circular ROI\n",
    "    mean_cover_fraction = clipped_land_cover.reduceRegion(\n",
    "        reducer=ee.Reducer.mean(),\n",
    "        geometry=roi,\n",
    "        scale=100\n",
    "    ).get('urban-coverfraction')\n",
    "\n",
    "    # Evaluate the server-side computation to get the actual result\n",
    "    mean_cover_fraction = mean_cover_fraction.getInfo()\n",
    "\n",
    "    # Classify as 'Urban' if the mean cover fraction exceeds a certain threshold (50), otherwise 'Rural'\n",
    "    if mean_cover_fraction >= 50:\n",
    "        land_class = 'Urban'\n",
    "    else:\n",
    "        land_class = 'Rural'\n",
    "\n",
    "    return land_class\n",
    "\n",
    "# # Example usage\n",
    "# latitude = 64  # Example latitude\n",
    "# longitude = -147.0060  # Example longitude\n",
    "# radius_km = 10  # Example radius in kilometers\n",
    "# classification = classify_area(latitude, longitude, radius_km)\n",
    "# print('Area classification:', classification)\n",
    "\n",
    "\n",
    "\n",
    "# Load the population density image\n",
    "population_density = ee.ImageCollection('CIESIN/GPWv411/GPW_Population_Density').first()\n",
    "\n",
    "# Define a function to calculate population within a radius circle\n",
    "def calculate_population(lat, lon, radius_meters):\n",
    "    # Create a point geometry\n",
    "    point = ee.Geometry.Point(lon, lat)\n",
    "    \n",
    "    # Buffer the point to create a circle with the specified radius\n",
    "    circle = point.buffer(radius_meters)\n",
    "    \n",
    "    # Clip the population density image to the circle\n",
    "    population_density_clip = population_density.clip(circle)\n",
    "    \n",
    "    # Calculate the total population within the circle\n",
    "    population_stats = population_density_clip.reduceRegion(\n",
    "        reducer=ee.Reducer.sum(),\n",
    "        geometry=circle,\n",
    "        scale=1000  # Resolution in meters\n",
    "    )\n",
    "    \n",
    "    # Extract the population count from the result\n",
    "    population_count = population_stats.get('population_density')\n",
    "    \n",
    "    return population_count.getInfo()\n",
    "\n",
    "\n",
    "# # Example usage: Calculate population within a 1 km radius circle at a specific location\n",
    "# latitude = 40.730610  # Example latitude (New York City)\n",
    "# longitude = -73.935242  # Example longitude (New York City)\n",
    "# radius_meters = 1000  # 1 km radius\n",
    "\n",
    "# population_within_circle = calculate_population(latitude, longitude, radius_meters)\n",
    "# print(\"Population within the circle:\", population_within_circle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6aa2593f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.api import OLS, add_constant\n",
    "\n",
    "\n",
    "\n",
    "fontnum = 22\n",
    "\n",
    "from pylib.smafit import smafit\n",
    "import statsmodels.formula.api as smf\n",
    "def LR_SMA(x,y):\n",
    "    import xarray as xr\n",
    "    \"\"\"\n",
    "    Input two xarray timeseries (x and y),\n",
    "    first do an inner aligh, then use SMA fitting to get the slope and intercept\n",
    "    \n",
    "    x,y: xarray 1D array\n",
    "        \n",
    "    \"\"\"\n",
    "#     x_align, y_align = xr.align( x, y, join=\"inner\")\n",
    "    x_align, y_align = x, y\n",
    "    s,i,stds,stdi,cis,cii = smafit( x_align, y_align, cl=0.95,robust=True)\n",
    "    return s,i\n",
    "\n",
    "\n",
    "def binscatter(x,y,bins):\n",
    "    import pandas as pd\n",
    "\n",
    "    df = pd.DataFrame({'X':x, 'Y':y})\n",
    "    my_bins = pd.cut(x, bins=bins)\n",
    "    data = df[['X', 'Y']].groupby(my_bins).agg(['mean', 'median', 'size','std'])\n",
    "#     data.plot.scatter(x=('x', 'mean'), y=('y', 'mean'))\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def add_text(ax, x, y, x_position=0.05, color=None):\n",
    "    \n",
    "    # Remove NaN values\n",
    "    valid_data = ~np.isnan(x) & ~np.isnan(y)\n",
    "    x_clean, y_clean = x[valid_data], y[valid_data]\n",
    "\n",
    "    # Perform SMA regression\n",
    "#     slope_sma, intercept_sma = LR_SMA(x_clean, y_clean)\n",
    "    slope_sma, _, _, _ = np.linalg.lstsq(x_clean[:,np.newaxis], y_clean[:,np.newaxis]);\n",
    "    slope_sma = slope_sma.squeeze()\n",
    "    intercept_sma = np.zeros((slope_sma.shape))\n",
    "    \n",
    "    print( slope_sma )\n",
    "    \n",
    "    # Calculate correlation coefficient\n",
    "    r = np.corrcoef(x_clean, y_clean)[0, 1]\n",
    "    rmse = np.sqrt(np.mean((y_clean - (slope_sma * x_clean + intercept_sma))**2))\n",
    "\n",
    "#     sma_equation = f\"Y = {slope_sma:.2f}X + {intercept_sma:.2f}\"\n",
    "    sma_equation = f\"Y = {slope_sma:.2f}X\"\n",
    "    n_points = f\"N = {len(x_clean)}\"\n",
    "    rmse_text = f\"RMSE = {rmse:.2f}\"\n",
    "    \n",
    "    # Add text to the top left corner of each panel\n",
    "    ax.text(x_position, 0.95, sma_equation, transform=ax.transAxes, fontsize=20, verticalalignment='top', color=color)\n",
    "    ax.text(x_position, 0.88, n_points, transform=ax.transAxes, fontsize=20, verticalalignment='top', color=color)\n",
    "    ax.text(x_position, 0.81, f\"R = {r:.3f}\", transform=ax.transAxes, fontsize=20, verticalalignment='top', color=color)\n",
    "    ax.text(x_position, 0.73, rmse_text, transform=ax.transAxes, fontsize=20, verticalalignment='top', color=color)\n",
    "\n",
    "    # Plot SMA regression line\n",
    "    line_color = color if color is not None else 'k'\n",
    "    ax.plot(x_clean, slope_sma * x_clean + intercept_sma,\\\n",
    "            linewidth=2, color=line_color, linestyle='-', zorder=10)\n",
    "\n",
    "\n",
    "\n",
    "def remove_outliers(x_values, y_values):\n",
    "    # Convert to numpy arrays if they aren't already\n",
    "    x = np.array(x_values)\n",
    "    y = np.array(y_values)\n",
    "\n",
    "    # Calculate Q1 and Q3 for y_values\n",
    "    Q1 = np.nanpercentile(y, 25)\n",
    "    Q3 = np.nanpercentile(y, 75)\n",
    "\n",
    "    # Calculate the IQR\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Define the bounds for outliers\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    print(Q1)\n",
    "\n",
    "    # Identify outlier indices\n",
    "    outlier_indices = (y < lower_bound) | (y > upper_bound)\n",
    "\n",
    "    # Set outliers to NaN\n",
    "    x[outlier_indices] = np.nan\n",
    "    y[outlier_indices] = np.nan\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def LR_zerointercept(x,y):\n",
    "    # Remove NaN values\n",
    "    valid_data = ~np.isnan(x) & ~np.isnan(y)\n",
    "    x_clean, y_clean = x[valid_data], y[valid_data]\n",
    "\n",
    "    # Perform SMA regression\n",
    "    slope, _, _, _ = np.linalg.lstsq(x_clean[:,np.newaxis], y_clean[:,np.newaxis]);\n",
    "    slope = float(slope.squeeze())\n",
    "    \n",
    "    return slope\n",
    "\n",
    "\n",
    "def desaturate_color(color_name, desaturation_factor=0.5):\n",
    "    \"\"\"\n",
    "    Reduce the saturation of a given color name.\n",
    "    \n",
    "    :param color_name: The name of the color (e.g., 'red', 'blue')\n",
    "    :param desaturation_factor: Factor to reduce saturation (between 0 and 1)\n",
    "    :return: The color with reduced saturation (in RGB format)\n",
    "    \"\"\"\n",
    "    import matplotlib.colors as mcolors\n",
    "    import colorsys\n",
    "    \n",
    "    # Convert the color name to RGB\n",
    "    rgb = mcolors.to_rgb(color_name)\n",
    "    \n",
    "    # Convert RGB to HSL\n",
    "    h, l, s = colorsys.rgb_to_hls(*rgb)\n",
    "    \n",
    "    # Reduce saturation\n",
    "    s *= desaturation_factor\n",
    "    \n",
    "    # Convert HSL back to RGB\n",
    "    desaturated_rgb = colorsys.hls_to_rgb(h, l, s)\n",
    "    \n",
    "    return desaturated_rgb\n",
    "\n",
    "def fill_vars(ds1, namestr):\n",
    "    \n",
    "    variableslist = info_fush_union[:,2]\n",
    "    # Items to remove\n",
    "    items_to_remove = {'time', 'lev', 'layer_center_height_bins'}\n",
    "    # New list excluding the items to remove\n",
    "    filtered_variableslist = [var for var in variableslist if var not in items_to_remove]\n",
    "    \n",
    "    if namestr == 'sitename_PID':\n",
    "        ds2 = ds1.copy()\n",
    "        # Add missing variables as NaNs\n",
    "        for var in filtered_variableslist:\n",
    "            if var not in list(ds2.variables):\n",
    "                ds2[var] = xr.full_like(ds2[list(ds2.data_vars)[0]], np.nan)\n",
    "\n",
    "    if namestr == 'sitename':\n",
    "        ds2 = {}\n",
    "        for var in filtered_variableslist:\n",
    "            if sep_sitename_ID(var)[0] in list(ds1.variables):\n",
    "                ds2[var] = ds1[ sep_sitename_ID(var)[0] ]\n",
    "            if sep_sitename_ID(var)[0] not in list(ds1.variables):\n",
    "                ds2[var] = xr.full_like(ds1[list(ds1.data_vars)[0]], np.nan)        \n",
    "        \n",
    "        ds2 = xr.Dataset(ds2)\n",
    "        \n",
    "    return ds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5408da90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New PGN data, more sites available!  ( Globally     107    sites that have both fuh and fus data in all time periods)\n",
    "\n",
    "info_fush_union = np.array([['-0.2046', '100.3195', 'Agam_P211', '865.0'],\n",
    "       ['-27.3493', '30.1438', 'Wakkerstroom_P159', '1760.0'],\n",
    "       ['-45.7833', '-67.45', 'ComodoroRivadavia_P124', '46.0'],\n",
    "       ['-6.8948', '107.5865', 'Bandung_P210', '752.0'],\n",
    "       ['1.299', '103.771', 'Singapore-NUS_P77', '77.0'],\n",
    "       ['13.7847', '100.54', 'Bangkok_P190', '60.0'],\n",
    "       ['18.3797', '-65.6184', 'Fajardo_P60', '66.0'],\n",
    "       ['19.1187', '-98.6552', 'Altzomoni_P65', '3985.0'],\n",
    "       ['19.3262', '-99.1761', 'MexicoCity-UNAM_P142', '2280.0'],\n",
    "       ['19.483', '-99.147', 'MexicoCity-Vallejo_P157', '2255.0'],\n",
    "       ['23.7284', '90.3982', 'Dhaka_P76', '34.0'],\n",
    "       ['28.309', '-16.4994', 'Izana_P121', '2360.0'],\n",
    "       ['28.309', '-16.4994', 'Izana_P209', '2360.0'],\n",
    "       ['29.6721', '-95.0647', 'LaPorteTX_P11', '22.0'],\n",
    "       ['29.6721', '-95.0647', 'LaPorteTX_P58', '22.0'],\n",
    "       ['29.6721', '-95.0647', 'LaPorteTX_P63', '22.0'],\n",
    "       ['29.72', '-95.34', 'HoustonTX_P25', '19.0'],\n",
    "       ['29.9011', '-95.3262', 'AldineTX_P61', '8.0'],\n",
    "       ['30.0965', '-94.7635', 'LibertyTX_P143', '3.0'],\n",
    "       ['32.1129', '34.8062', 'Tel-Aviv_P182', '76.0'],\n",
    "       ['32.7316', '-97.1142', 'ArlingtonTX_P207', '15.0'],\n",
    "       ['33.5491', '130.366', 'Fukuoka_P199', '55.0'],\n",
    "       ['33.6479', '72.9896', 'Islamabad-NUST_P73', '555.0'],\n",
    "       ['33.779', '-84.3958', 'AtlantaGA-SouthDeKalb_P237', '310.0'],\n",
    "       ['33.779', '-84.3958', 'AtlantaGA_P173', '310.0'],\n",
    "       ['34.3819', '-117.6813', 'WrightwoodCA_P68', '2207.0'],\n",
    "       ['34.719', '135.29', 'Kobe_P198', '23.0'],\n",
    "       ['34.7252', '-86.6464', 'HuntsvilleAL_P66', '221.0'],\n",
    "       ['34.96', '-117.8811', 'EdwardsCA_P74', '692.0'],\n",
    "       ['35.1517', '136.9721', 'Nagoya_P197', '117.0'],\n",
    "       ['35.2353', '129.0825', 'Busan_P20', '71.0'],\n",
    "       ['35.3207', '139.6508', 'Yokosuka_P146', '5.0'],\n",
    "       ['35.5745', '129.1896', 'Ulsan_P150', '38.0'],\n",
    "       ['35.62', '139.3834', 'Tokyo-TMU_P194', '135.0'],\n",
    "       ['35.9708', '-79.0933', 'ChapelHillNC_P70', '50.0'],\n",
    "       ['36.0506', '140.1202', 'Tsukuba-NIES-West_P163', '30.0'],\n",
    "       ['36.0513', '140.121', 'Tsukuba-NIES_P176', '45.0'],\n",
    "       ['36.0661', '140.1244', 'Tsukuba_P193', '51.0'],\n",
    "       ['36.7769', '126.4938', 'Seosan_P164', '25.0'],\n",
    "       ['37.0203', '-76.3366', 'HamptonVA-HU_P156', '19.0'],\n",
    "       ['37.164', '-3.605', 'Granada_P238', '680.0'],\n",
    "       ['37.326', '-77.2057', 'CharlesCityVA_P31', '6.0'],\n",
    "       ['37.3325', '-121.8821', 'SanJoseCA_P181', '69.0'],\n",
    "       ['37.42', '-122.0568', 'MountainViewCA_P34', '50.0'],\n",
    "       ['37.458', '126.951', 'Seoul-SNU_P149', '116.0'],\n",
    "       ['37.5644', '126.934', 'Seoul_P54', '86.0'],\n",
    "       ['37.5689', '126.6375', 'Incheon-ESC_P189', '6.0'],\n",
    "       ['37.8439', '-75.4775', 'WallopsIslandVA_P40', '11.0'],\n",
    "       ['37.913', '-122.336', 'RichmondCA_P52', '5.0'],\n",
    "       ['37.9878', '23.775', 'Athens-NOA_P119', '130.0'],\n",
    "       ['38.9218', '-77.0124', 'WashingtonDC_P140', '58.0'],\n",
    "       ['38.9926', '-76.8396', 'GreenbeltMD_P2', '90.0'],\n",
    "       ['38.9926', '-76.8396', 'GreenbeltMD_P32', '90.0'],\n",
    "       ['39.0553', '-76.8783', 'BeltsvilleMD_P80', '73.0'],\n",
    "       ['39.1022', '-96.6096', 'ManhattanKS_P165', '346.0'],\n",
    "       ['39.99', '-105.26', 'BoulderCO_P57', '1660.0'],\n",
    "       ['39.9919', '-75.0811', 'PhiladelphiaPA_P166', '6.0'],\n",
    "       ['40.0048', '116.3786', 'Beijing-RADI_P171', '59.0'],\n",
    "       ['40.0375', '-105.242', 'BoulderCO-NCAR_P204', '1616.0'],\n",
    "       ['40.1074', '-74.8824', 'BristolPA_P134', '10.0'],\n",
    "       ['40.4622', '-74.4294', 'NewBrunswickNJ_P69', '19.0'],\n",
    "       ['40.4655', '-79.9608', 'PittsburghPA_P187', '265.0'],\n",
    "       ['40.548', '-112.07', 'SouthJordanUT_P139', '1582.0'],\n",
    "       ['40.6336', '22.9561', 'Thessaloniki_P240', '60.0'],\n",
    "       ['40.6703', '-74.1261', 'BayonneNJ_P38', '3.0'],\n",
    "       ['40.7344', '-111.8722', 'SaltLakeCityUT-Hawthorne_P72', '1306.0'],\n",
    "       ['40.7361', '-73.8215', 'QueensNY_P55', '25.0'],\n",
    "       ['40.7663', '-111.8478', 'SaltLakeCityUT_P154', '1455.0'],\n",
    "       ['40.8153', '-73.9505', 'ManhattanNY-CCNY_P135', '34.0'],\n",
    "       ['40.8679', '-73.8781', 'BronxNY_P180', '31.0'],\n",
    "       ['40.9635', '-73.1402', 'OldFieldNY_P51', '3.0'],\n",
    "       ['41.1183', '-73.3367', 'WestportCT_P177', '4.0'],\n",
    "       ['41.2568', '-72.5533', 'MadisonCT_P186', '3.0'],\n",
    "       ['41.3014', '-72.9029', 'NewHavenCT_P64', '4.0'],\n",
    "       ['41.3758', '-72.1004', 'NewLondonCT_P236', '30.0'],\n",
    "       ['41.8213', '-73.2973', 'CornwallCT_P179', '505.0'],\n",
    "       ['41.8403', '12.6475', 'Rome-ISAC_P115', '117.0'],\n",
    "       ['41.841', '-71.361', 'EastProvidenceRI_P185', '15.0'],\n",
    "       ['41.9017', '12.5158', 'Rome-SAP_P117', '75.0'],\n",
    "       ['41.9017', '12.5158', 'Rome-SAP_P138', '75.0'],\n",
    "       ['42.1057', '12.6402', 'Rome-IIA_P138', '92.0'],\n",
    "       ['42.2929', '-83.0731', 'Windsor-West_P208', '180.0'],\n",
    "       ['42.3026', '-83.1068', 'SWDetroitMI_P147', '178.0'],\n",
    "       ['42.3067', '-83.1488', 'DearbornMI_P39', '181.0'],\n",
    "       ['42.38', '-71.11', 'CambridgeMA_P26', '60.0'],\n",
    "       ['42.4746', '-70.9708', 'LynnMA_P153', '52.0'],\n",
    "       ['42.8625', '-71.3801', 'LondonderryNH_P183', '108.0'],\n",
    "       ['43.0727', '141.3459', 'Sapporo_P196', '46.0'],\n",
    "       ['43.561', '-70.2073', 'CapeElizabethME_P184', '24.0'],\n",
    "       ['43.5773', '104.4191', 'Dalanzadgad_P217', '1466.0'],\n",
    "       ['43.7094', '-79.5435', 'Toronto-West_P108', '141.0'],\n",
    "       ['43.781', '-79.468', 'Downsview_P104', '187.0'],\n",
    "       ['43.781', '-79.468', 'Downsview_P170', '187.0'],\n",
    "       ['43.7843', '-79.1874', 'Toronto-Scarborough_P145', '137.0'],\n",
    "       ['44.23', '-79.78', 'Egbert_P108', '251.0'],\n",
    "       ['46.8', '9.83', 'Davos_P120', '1590.0'],\n",
    "       ['47.2643', '11.3852', 'Innsbruck_P106', '616.0'],\n",
    "       ['47.2643', '11.3852', 'Innsbruck_P110', '616.0'],\n",
    "       ['47.9188', '106.848', 'Ulaanbaatar_P216', '1305.0'],\n",
    "       ['50.798', '4.358', 'Brussels-Uccle_P162', '107.0'],\n",
    "       ['50.908', '6.413', 'Juelich_P30', '94.0'],\n",
    "       ['50.9389', '6.9787', 'Cologne_P67', '50.0'],\n",
    "       ['53.0813', '8.8126', 'Bremen_P21', '50.0'],\n",
    "       ['60.2037', '24.9612', 'Helsinki_P105', '97.0'],\n",
    "       ['64.8594', '-147.8499', 'FairbanksAK_P174', '227.0'],\n",
    "       ['7.342', '134.4722', 'Palau_P131', '23.0'],\n",
    "       ['78.9233', '11.9299', 'NyAlesund_P152', '18.0']], dtype='<U32')\n",
    "\n",
    "info_fush_union = np.append(info_fush_union, np.empty((len(info_fush_union), 1), dtype=object), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe6a58ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_fush_union[:, 4] = \"\"\n",
    "radius_km = 5\n",
    "\n",
    "#  reprocess info array if needed\n",
    "for i, site_info in enumerate(info_fush_union):\n",
    "    lat = float(site_info[0])\n",
    "    lon = float(site_info[1])\n",
    "    landclass = classify_area(lat, lon, radius_km)\n",
    "    info_fush_union[i, 4] += landclass\n",
    "\n",
    "\n",
    "    \n",
    "urban_rows = info_fush_union[(info_fush_union[:, 4] == 'Urban') ]\n",
    "rural_rows = info_fush_union[(info_fush_union[:, 4] == 'Rural') ]\n",
    "\n",
    "urban_rows_NA = filter_sites_in_region( urban_rows , \"NA\" )\n",
    "rural_rows_NA = filter_sites_in_region( rural_rows , \"NA\" )\n",
    "urbanrural_rows_NA = filter_sites_in_region( info_fush_union , \"NA\" )\n",
    "\n",
    "urban_rows_EU = filter_sites_in_region( urban_rows , \"EU\" )\n",
    "rural_rows_EU = filter_sites_in_region( rural_rows , \"EU\" )\n",
    "urbanrural_rows_EU = filter_sites_in_region( info_fush_union , \"EU\" )\n",
    "\n",
    "urban_rows_AS = filter_sites_in_region( urban_rows , \"AS\" )\n",
    "rural_rows_AS = filter_sites_in_region( rural_rows , \"AS\" )\n",
    "urbanrural_rows_AS = filter_sites_in_region( info_fush_union , \"AS\" )\n",
    "\n",
    "urban_rows_Global = filter_sites_in_region( urban_rows , \"Global\" )\n",
    "rural_rows_Global = filter_sites_in_region( rural_rows , \"Global\" )\n",
    "urbanrural_rows_Global = filter_sites_in_region( info_fush_union , \"Global\" )\n",
    "\n",
    "#   reprocess info array if needed\n",
    "for i, site_info in enumerate(info_fush_union):\n",
    "    site_name = site_info[2]\n",
    "    if any(urbanrural_rows_AS[:, 2] == site_name):\n",
    "        info_fush_union[i, 4] += \" AS\"\n",
    "    elif any(urbanrural_rows_EU[:, 2] == site_name):\n",
    "        info_fush_union[i, 4] += \" EU\"\n",
    "    elif any(urbanrural_rows_NA[:, 2] == site_name):\n",
    "        info_fush_union[i, 4] += \" NA\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9c147d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  detailed regions of NA\n",
    "\n",
    "urban_rows_EUS = filter_sites_in_region( urban_rows , \"EUS\" )\n",
    "rural_rows_EUS = filter_sites_in_region( rural_rows , \"EUS\" )\n",
    "urbanrural_rows_EUS = filter_sites_in_region( info_fush_union , \"EUS\" )\n",
    "\n",
    "urban_rows_SEUS = filter_sites_in_region( urban_rows , \"SEUS\" )\n",
    "rural_rows_SEUS = filter_sites_in_region( rural_rows , \"SEUS\" )\n",
    "urbanrural_rows_SEUS = filter_sites_in_region( info_fush_union , \"SEUS\" )\n",
    "\n",
    "urban_rows_NEUS = filter_sites_in_region( urban_rows , \"NEUS\" )\n",
    "rural_rows_NEUS = filter_sites_in_region( rural_rows , \"NEUS\" )\n",
    "urbanrural_rows_NEUS = filter_sites_in_region( info_fush_union , \"NEUS\" )\n",
    "\n",
    "urban_rows_SUS = filter_sites_in_region( urban_rows , \"SUS\" )\n",
    "rural_rows_SUS = filter_sites_in_region( rural_rows , \"SUS\" )\n",
    "urbanrural_rows_SUS = filter_sites_in_region( info_fush_union , \"SUS\" )\n",
    "\n",
    "urban_rows_MWUS = filter_sites_in_region( urban_rows , \"MWUS\" )\n",
    "rural_rows_MWUS = filter_sites_in_region( rural_rows , \"MWUS\" )\n",
    "urbanrural_rows_MWUS = filter_sites_in_region( info_fush_union , \"MWUS\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c8c372d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add population density (people per km2)\n",
    "if info_fush_union.shape[1] <= 5:\n",
    "    info_fush_union = np.insert(info_fush_union, 5, values=\"\", axis=1)\n",
    "    \n",
    "radius_meter = 5*1000     # function uses meter\n",
    "\n",
    "#  reprocess info array if needed\n",
    "for i, site_info in enumerate(info_fush_union):\n",
    "    lat = float(site_info[0])\n",
    "    lon = float(site_info[1])\n",
    "    populationdensity = calculate_population(lat, lon, radius_meter)\n",
    "    info_fush_union[i, 5] += str(populationdensity)\n",
    "    info_fush_union[i, 5] = float(info_fush_union[i, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56ae42d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert info_fush_union to dataframe\n",
    "\n",
    "info_fush_union_df = pd.DataFrame(info_fush_union)\n",
    "column_names = ['lat', 'lon', 'sitename_PID', 'alt_m', 'landtype_region','population_5kmcircle']\n",
    "# Convert to DataFrame with column names\n",
    "info_fush_union_df = pd.DataFrame(info_fush_union, columns=column_names)\n",
    "\n",
    "# Convert specific columns from string to float\n",
    "columns_to_convert = ['lat', 'lon', 'alt_m', 'population_5kmcircle']\n",
    "for col_idx in columns_to_convert:\n",
    "    info_fush_union_df[col_idx] = info_fush_union_df[col_idx].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be19e625",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673b0cc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8349a4b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6853e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a9ba0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f570ab36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c8b64c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load GEOS-CF data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_geoscf_data_csv(year, start_date, end_date):\n",
    "    folder_path = '/import/GREENING/tzhao/GEOSCF_subset/'\n",
    "    prefix = f'GEOSCF'\n",
    "    suffix = f'{year}-{start_date}_{year}-{end_date}.csv'\n",
    "\n",
    "    hcho_data = pd.read_csv(folder_path + prefix + '_HCHO_' + suffix, sep=',', index_col=0)\n",
    "    hcho_data.index = pd.to_datetime(hcho_data.index)\n",
    "    hcho_data = hcho_data.drop_duplicates().to_xarray()\n",
    "    hcho_data = hcho_data.rename({'datetime_UTC': 'time'})\n",
    "\n",
    "    tropcol_hcho_data = pd.read_csv(folder_path + prefix + '_TROPCOL_HCHO_' + suffix, sep=',', index_col=0)\n",
    "    tropcol_hcho_data.index = pd.to_datetime(tropcol_hcho_data.index)\n",
    "    tropcol_hcho_data = tropcol_hcho_data.drop_duplicates().to_xarray()\n",
    "    tropcol_hcho_data = tropcol_hcho_data.rename({'datetime_UTC': 'time'})\n",
    "\n",
    "    totcol_hcho_data = pd.read_csv(folder_path + prefix + '_TOTCOL_HCHO_' + suffix, sep=',', index_col=0)\n",
    "    totcol_hcho_data.index = pd.to_datetime(totcol_hcho_data.index)\n",
    "    totcol_hcho_data = totcol_hcho_data.drop_duplicates().to_xarray()\n",
    "    totcol_hcho_data = totcol_hcho_data.rename({'datetime_UTC': 'time'})\n",
    "\n",
    "    return hcho_data, tropcol_hcho_data, totcol_hcho_data\n",
    "\n",
    "GEOSCF_HCHO_2021_warm, GEOSCF_TROPCOL_HCHO_2021_warm, GEOSCF_TOTCOL_HCHO_2021_warm = load_geoscf_data_csv('2021', '05-01', '08-31')\n",
    "GEOSCF_HCHO_2022_warm, GEOSCF_TROPCOL_HCHO_2022_warm, GEOSCF_TOTCOL_HCHO_2022_warm = load_geoscf_data_csv('2022', '05-01', '08-31')\n",
    "GEOSCF_HCHO_2023_warm, GEOSCF_TROPCOL_HCHO_2023_warm, GEOSCF_TOTCOL_HCHO_2023_warm = load_geoscf_data_csv('2023', '05-01', '08-31')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "be64b4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_geoscf_vp_csv(year, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Read vertical profile *.csv files from a directory, concatenate them,\n",
    "    and load into a netCDF dataset.\n",
    "\n",
    "    Parameters:\n",
    "    year (int): Year for which to load data.\n",
    "    start_date (str): Start date of the time range (format: MM-DD).\n",
    "    end_date (str): End date of the time range (format: MM-DD).\n",
    "\n",
    "    Returns:\n",
    "    xarray.Dataset: NetCDF dataset containing concatenated data.\n",
    "    \"\"\"\n",
    "\n",
    "    directory = '/import/GREENING/tzhao/GEOSCF_subset_opendap/'\n",
    "    all_dataframes = []\n",
    "\n",
    "    start_date = pd.to_datetime(f\"{year}-{start_date}\")\n",
    "    end_date = pd.to_datetime(f\"{year}-{end_date}\")\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if 'chm_tavg_1hr_g1440x721_v36__HCHO_merged_' + str(year) in filename and filename.endswith('.csv'):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            df = pd.read_csv(filepath)\n",
    "            df['time'] = pd.to_datetime(df['time'])\n",
    "            # Set minute, second, and microsecond to zero\n",
    "            df['time'] = df['time'].dt.floor('H')  # Set to the beginning of the hour\n",
    "            df = df[(df['time'] >= start_date) & (df['time'] <= end_date)]\n",
    "            all_dataframes.append(df)\n",
    "\n",
    "    concatenated_df = pd.concat(all_dataframes)\n",
    "    concatenated_df = concatenated_df.drop_duplicates(subset=['time', 'lev'], keep='first')\n",
    "    concatenated_df['time'] = pd.to_datetime(concatenated_df['time'])\n",
    "    \n",
    "    xarray_dataset = xr.Dataset.from_dataframe(concatenated_df.set_index(['time', 'lev']))\n",
    "    xarray_dataset = xarray_dataset.assign_coords(lev=H_geoscf)\n",
    "\n",
    "    return xarray_dataset\n",
    "\n",
    "\n",
    "GEOSCF_HCHO_v36_2021_warm  = load_geoscf_vp_csv('2021', '05-01', '08-31')\n",
    "GEOSCF_HCHO_v36_2022_warm  = load_geoscf_vp_csv('2022', '05-01', '08-31')\n",
    "GEOSCF_HCHO_v36_2023_warm  = load_geoscf_vp_csv('2023', '05-01', '08-31')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22b24e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_geoscf_ozone_csv(year, start_date, end_date):\n",
    "    folder_path = '/import/GREENING/tzhao/GEOSCF_subset/'\n",
    "    prefix = f'GEOSCF'\n",
    "    suffix = f'{year}-{start_date}_{year}-{end_date}.csv'\n",
    "\n",
    "    hcho_data = pd.read_csv(folder_path + prefix + '_O3_' + suffix, sep=',', index_col=0)\n",
    "    hcho_data.index = pd.to_datetime(hcho_data.index)\n",
    "    hcho_data = hcho_data.drop_duplicates().to_xarray()\n",
    "    hcho_data = hcho_data.rename({'datetime_UTC': 'time'})\n",
    "    \n",
    "    return hcho_data\n",
    "    \n",
    "GEOSCF_O3_2021_warm = load_geoscf_ozone_csv('2021', '05-01', '08-31')\n",
    "GEOSCF_O3_2022_warm = load_geoscf_ozone_csv('2022', '05-01', '08-31')\n",
    "GEOSCF_O3_2023_warm = load_geoscf_ozone_csv('2023', '05-01', '08-31')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e1d19970",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEOSCF_HCHO_2021to2023_warm = xr.concat( [GEOSCF_HCHO_2021_warm,GEOSCF_HCHO_2022_warm,GEOSCF_HCHO_2023_warm] , dim='time')\n",
    "\n",
    "GEOSCF_TROPCOL_HCHO_2021to2023_warm = xr.concat( [GEOSCF_TROPCOL_HCHO_2021_warm,GEOSCF_TROPCOL_HCHO_2022_warm,\n",
    "                                   GEOSCF_TROPCOL_HCHO_2023_warm] , dim='time')\n",
    "\n",
    "GEOSCF_TOTCOL_HCHO_2021to2023_warm = xr.concat( [GEOSCF_TOTCOL_HCHO_2021_warm,GEOSCF_TOTCOL_HCHO_2022_warm,\n",
    "                                   GEOSCF_TOTCOL_HCHO_2023_warm] , dim='time')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "52cf392a",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEOSCF_HCHO_v36_2021to2023_warm = xr.concat( [GEOSCF_HCHO_v36_2021_warm,\\\n",
    "                                              GEOSCF_HCHO_v36_2022_warm,\\\n",
    "                                              GEOSCF_HCHO_v36_2023_warm] , dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3739e4-b107-420b-9372-d6397b02ca1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb4fbbc-aed0-4390-b3c6-5bd0024fa9bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "054ba8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load PGN data \n",
    "\n",
    "def fill_missing_variables(ds):\n",
    "\n",
    "    for site in info_fush_union[:, 2]:\n",
    "        # Check if the site coordinate is present in the dataset\n",
    "        if site not in list(ds.variables):\n",
    "            # If site is not present, create a new DataArray with NaN values\n",
    "            nan_values = np.full(len(ds.time), np.nan)\n",
    "            new_data_array = xr.DataArray(nan_values, coords=[ds.time], dims=['time'], name=site)\n",
    "            # Concatenate the new DataArray with the original dataset along the 'site' dimension\n",
    "            ds[site]=new_data_array\n",
    "            \n",
    "    return ds\n",
    "\n",
    "\n",
    "# Define the years\n",
    "years = ['2021', '2022', '2023']\n",
    "\n",
    "# Define the filenames common part\n",
    "filenames = [\n",
    "    \"PGN_fuh_HCHOTropVCD\",\n",
    "    \"PGN_fuh_HCHOSurfConc\",\n",
    "    \"PGN_fus_HCHOVCD\"\n",
    "]\n",
    "\n",
    "tmp_ds_warm  = {}\n",
    "tmp_ds_cold = {}\n",
    "\n",
    "for year in years:\n",
    "    for filename in filenames:\n",
    "\n",
    "        # Construct the full filename\n",
    "        full_filename = os.path.join(jndir_HCHOdiurnal_data, f\"{filename}_{year}05_08.nc\")\n",
    "        # Open the dataset and store it in the dictionary\n",
    "        tmp = xr.open_dataset(full_filename, engine=\"netcdf4\")\n",
    "        tmp = fill_missing_variables( tmp ).copy()\n",
    "        tmp_ds_warm[f\"{filename}_{year}_warm\"] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60bcfa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "PGN_fuh_HCHOTropVCD_2021to2023_warm = xr.concat([tmp_ds_warm['PGN_fuh_HCHOTropVCD_2021_warm'],\\\n",
    "                                                 tmp_ds_warm['PGN_fuh_HCHOTropVCD_2022_warm'],\\\n",
    "                                                 tmp_ds_warm['PGN_fuh_HCHOTropVCD_2023_warm']], dim='time')\n",
    "\n",
    "PGN_fuh_HCHOSurfConc_2021to2023_warm = xr.concat([tmp_ds_warm['PGN_fuh_HCHOSurfConc_2021_warm'],\\\n",
    "                                                 tmp_ds_warm['PGN_fuh_HCHOSurfConc_2022_warm'],\\\n",
    "                                                 tmp_ds_warm['PGN_fuh_HCHOSurfConc_2023_warm']], dim='time')\n",
    "\n",
    "PGN_fus_HCHOVCD_2021to2023_warm = xr.concat([tmp_ds_warm['PGN_fus_HCHOVCD_2021_warm'],\\\n",
    "                                                 tmp_ds_warm['PGN_fus_HCHOVCD_2022_warm'],\\\n",
    "                                                 tmp_ds_warm['PGN_fus_HCHOVCD_2023_warm']], dim='time')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f94eae7c-bfaf-46bc-9d63-720f96a4a6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修改第二维名称函数\n",
    "def rename_second_dim(obj, new_dim_name):\n",
    "    if len(obj.dims) > 1:\n",
    "        return obj.rename({obj.dims[1]: new_dim_name})\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "568bae0c-c73c-4e55-957a-c9cfaec9eb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HCHO profiles\n",
    "\n",
    "\n",
    "years = ['2021', '2022', '2023']\n",
    "\n",
    "# Define the filenames common part\n",
    "filenames = [\n",
    "    \"PGN_Pandora_fuh_HCHOdVCDprofile\",\n",
    "]\n",
    "\n",
    "tmp_ds_warm = {}\n",
    "# tmp_ds_cold = {}\n",
    "\n",
    "for year in years:\n",
    "    for filename in filenames:\n",
    "        # Construct the full filename\n",
    "        full_filename = os.path.join(jndir_HCHOdiurnal_data, f\"{filename}_{year}05_08.FINAL6.unscale.nc\")\n",
    "        \n",
    "        # Open the dataset and store it in the dictionary\n",
    "        tmp = xr.open_dataset(full_filename, engine=\"netcdf4\")\n",
    "        tmp = fill_missing_variables(tmp).copy()\n",
    "\n",
    "        # 统一修改 data_vars 和 coords 中的第二维名称\n",
    "        tmp = tmp.map(lambda v: rename_second_dim(v, \"layer_center_height_bins\"))\n",
    "\n",
    "        # 删除名字以 \"boxheight_binned_\" 开头的 coordinates\n",
    "        coords_to_drop = [coord for coord in tmp.coords if coord.startswith(\"layer_center_height_binned_\")]\n",
    "        tmp = tmp.drop_vars(coords_to_drop)\n",
    "\n",
    "        tmp_ds_warm[f\"{filename}_{year}_warm\"] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7ca6675e-af0b-41a5-824e-d5c89e626dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取所有数据集中出现过的坐标集合\n",
    "all_coords = set()\n",
    "for key in tmp_ds_warm:\n",
    "    all_coords.update(tmp_ds_warm[key].coords)\n",
    "\n",
    "# 遍历数据集，补充缺失的坐标\n",
    "for key in tmp_ds_warm:\n",
    "    missing_coords = all_coords - set(tmp_ds_warm[key].coords)\n",
    "    for coord in missing_coords:\n",
    "        # 为缺失的坐标填充 NaN\n",
    "        tmp_ds_warm[key] = tmp_ds_warm[key].assign_coords({coord: np.nan})\n",
    "\n",
    "# 合并数据集\n",
    "PGN_fuh_HCHOdVCDprofile_2021to2023_warm = xr.concat(\n",
    "    [tmp_ds_warm['PGN_Pandora_fuh_HCHOdVCDprofile_2021_warm'],\n",
    "     tmp_ds_warm['PGN_Pandora_fuh_HCHOdVCDprofile_2022_warm'],\n",
    "     tmp_ds_warm['PGN_Pandora_fuh_HCHOdVCDprofile_2023_warm']],\n",
    "    dim='time'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5260a836-1f0d-4083-97ba-cadefed1db58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 需要提取的前缀类型\n",
    "prefixes = [ \"sfc_hourly_\"]\n",
    "\n",
    "# **1. 获取所有站点名称（即 tmp_ds_warm 的 variable name 列表）**\n",
    "all_sites = set()\n",
    "for year in ['2021', '2022', '2023']:\n",
    "    dataset_key = f'PGN_Pandora_fuh_HCHOdVCDprofile_{year}_warm'\n",
    "    ds = tmp_ds_warm[dataset_key]\n",
    "    \n",
    "    # 获取所有 variable names（站点名）\n",
    "    all_sites.update(ds.data_vars.keys())\n",
    "\n",
    "# **2. 创建存储不同前缀的 xarray.Dataset**\n",
    "datasets_by_prefix = {prefix: {} for prefix in prefixes}\n",
    "\n",
    "# **3. 遍历每个年份数据集，提取不同前缀的数据**\n",
    "for year in ['2021', '2022', '2023']:\n",
    "    dataset_key = f'PGN_Pandora_fuh_HCHOdVCDprofile_{year}_warm'\n",
    "    ds = tmp_ds_warm[dataset_key]\n",
    "    \n",
    "    # 处理每个前缀类型\n",
    "    for prefix in prefixes:\n",
    "        # 提取当前前缀的坐标数据\n",
    "        coord_vars = {var.replace(prefix, \"\"): ds.coords[var] for var in ds.coords if var.startswith(prefix)}\n",
    "\n",
    "        # **4. 确保所有站点都存在**\n",
    "        new_vars = {}\n",
    "        for site in all_sites:\n",
    "            if site in coord_vars:\n",
    "                new_vars[site] = coord_vars[site]\n",
    "            else:\n",
    "                # 该站点没有该类型数据，填充 NaN\n",
    "                new_vars[site] = xr.DataArray(np.full(ds.dims['time'], np.nan), dims=['time'])\n",
    "\n",
    "        # **5. 创建新的 xarray.Dataset**\n",
    "        prefix_ds = xr.Dataset(new_vars, coords={'time': ds['time']})\n",
    "        \n",
    "        # 存储该年前缀的数据\n",
    "        datasets_by_prefix[prefix][year] = prefix_ds\n",
    "\n",
    "# **6. 统一所有数据集的坐标**\n",
    "for prefix in prefixes:\n",
    "    all_coords = set()\n",
    "    for year in datasets_by_prefix[prefix]:\n",
    "        all_coords.update(datasets_by_prefix[prefix][year].coords)\n",
    "\n",
    "    # 确保所有数据集具有相同的坐标\n",
    "    for year in datasets_by_prefix[prefix]:\n",
    "        missing_coords = all_coords - set(datasets_by_prefix[prefix][year].coords)\n",
    "        for coord in missing_coords:\n",
    "            datasets_by_prefix[prefix][year] = datasets_by_prefix[prefix][year].assign_coords({coord: np.nan})\n",
    "\n",
    "# **7. 合并三年的数据**\n",
    "final_datasets = {}\n",
    "for prefix in prefixes:\n",
    "    final_datasets[prefix] = xr.concat(\n",
    "        [datasets_by_prefix[prefix]['2021'], datasets_by_prefix[prefix]['2022'], datasets_by_prefix[prefix]['2023']],\n",
    "        dim='time'\n",
    "    )\n",
    "\n",
    "# **8. 结果存储**\n",
    "PGN_fuh_HCHOSurfConc_2021to2023_warm = final_datasets[\"sfc_hourly_\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228263e5-d68d-4167-a6d4-fd45ab74fdf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230c8474",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd1ec3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_geoscf_met_csv(var, year, start_date, end_date):\n",
    "    directory = '/import/GREENING/tzhao/GEOSCF_subset_opendap/'\n",
    "    all_dataframes = []\n",
    "\n",
    "    start_date = pd.to_datetime(f\"{year}-{start_date}\")\n",
    "    end_date = pd.to_datetime(f\"{year}-{end_date}\")\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if 'met_tavg_1hr_g1440x721_v36__'+ str(var) +'_merged_' + str(year) in filename and filename.endswith('.csv'):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            df = pd.read_csv(filepath)\n",
    "            df['time'] = pd.to_datetime(df['time'])\n",
    "            # Set minute, second, and microsecond to zero\n",
    "            df['time'] = df['time'].dt.floor('H')  # Set to the beginning of the hour\n",
    "            df = df[(df['time'] >= start_date) & (df['time'] <= end_date)]\n",
    "            all_dataframes.append(df)\n",
    "\n",
    "    concatenated_df = pd.concat(all_dataframes)\n",
    "    concatenated_df = concatenated_df.drop_duplicates(subset=['time', 'lev'], keep='first')\n",
    "    concatenated_df['time'] = pd.to_datetime(concatenated_df['time'])\n",
    "    \n",
    "    xarray_dataset = xr.Dataset.from_dataframe(concatenated_df.set_index(['time', 'lev']))\n",
    "    xarray_dataset = xarray_dataset.assign_coords(lev=H_geoscf)\n",
    "\n",
    "    return xarray_dataset\n",
    "\n",
    "\n",
    "\n",
    "def load_geoscf_vp_met_csv(var, year, start_date, end_date):\n",
    "\n",
    "    directory = '/import/GREENING/tzhao/GEOSCF_subset_opendap/'\n",
    "    all_dataframes = []\n",
    "\n",
    "    start_date = pd.to_datetime(f\"{year}-{start_date}\")\n",
    "    end_date = pd.to_datetime(f\"{year}-{end_date}\")\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if 'met_tavg_1hr_g1440x721_v36__'+ str(var) +'_merged_' + str(year) in filename and filename.endswith('.csv'):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            df = pd.read_csv(filepath)\n",
    "            df['time'] = pd.to_datetime(df['time'])\n",
    "            # Set minute, second, and microsecond to zero\n",
    "            df['time'] = df['time'].dt.floor('H')  # Set to the beginning of the hour\n",
    "            df = df[(df['time'] >= start_date) & (df['time'] <= end_date)]\n",
    "            all_dataframes.append(df)\n",
    "\n",
    "    concatenated_df = pd.concat(all_dataframes)\n",
    "    concatenated_df = concatenated_df.drop_duplicates(subset=['time', 'lev'], keep='first')\n",
    "    concatenated_df['time'] = pd.to_datetime(concatenated_df['time'])\n",
    "    \n",
    "    xarray_dataset = xr.Dataset.from_dataframe(concatenated_df.set_index(['time', 'lev']))\n",
    "    xarray_dataset = xarray_dataset.assign_coords(lev=H_geoscf)\n",
    "\n",
    "    return xarray_dataset\n",
    "\n",
    "\n",
    "\n",
    "GEOSCF_T_v36_2021_warm = load_geoscf_met_csv( 'T' ,'2021', '05-01', '08-31')\n",
    "GEOSCF_T_v36_2022_warm = load_geoscf_met_csv( 'T' ,'2022', '05-01', '08-31')\n",
    "GEOSCF_T_v36_2023_warm = load_geoscf_met_csv( 'T' ,'2023', '05-01', '08-31')\n",
    "\n",
    "\n",
    "GEOSCF_T_v36_2021to2023_warm = xr.concat( [GEOSCF_T_v36_2021_warm,GEOSCF_T_v36_2022_warm,GEOSCF_T_v36_2023_warm] , dim='time')\n",
    "GEOSCF_T_v36_2021to2023_warm = GEOSCF_T_v36_2021to2023_warm.drop_vars(['time.1', 'lev.1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abda90c1-9de0-4e20-8d32-53831fe62b9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "32f266f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     PGN HCHO\n",
    "# --------------------------------------------------\n",
    "# Perform linear interpolation\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "\n",
    "interp_pressure = interp1d(H_geoscf[H_geoscf<=3], P_geoscf[H_geoscf<=3], kind='linear', fill_value='extrapolate')\n",
    "\n",
    "# Interpolate pressure for the second altitude list\n",
    "layer_center_P_bins = interp_pressure(PGN_fuh_HCHOdVCDprofile_2021to2023_warm.layer_center_height_bins)\n",
    "# Select levels less than 3\n",
    "tmp_below3km = GEOSCF_T_v36_2021to2023_warm.sel(lev=GEOSCF_T_v36_2021to2023_warm['lev'].where(GEOSCF_T_v36_2021to2023_warm['lev'] < 3, drop=True))\n",
    "\n",
    "GEOSCF_T_v36_2021to2023_warm_PGNlev = tmp_below3km.interp(lev=PGN_fuh_HCHOdVCDprofile_2021to2023_warm.layer_center_height_bins.values, method='linear')\n",
    "\n",
    "#  calculate conversion factor: \n",
    "factor_PGN_HCHOvp_ppbv_2021to2023_warm = ( 8.3145 * GEOSCF_T_v36_2021to2023_warm_PGNlev ) / (6.02e11 * layer_center_P_bins * 1e-1)\n",
    "\n",
    "\n",
    "# convert unit for PGN\n",
    "factor_PGN_HCHOvp_ppbv_2021to2023_warm = factor_PGN_HCHOvp_ppbv_2021to2023_warm.map(lambda v: rename_second_dim(v, \"layer_center_height_bins\"))\n",
    "PGN_fuh_HCHOdVCDprofile_2021to2023_warm_ppbv = PGN_fuh_HCHOdVCDprofile_2021to2023_warm * factor_PGN_HCHOvp_ppbv_2021to2023_warm\n",
    "PGN_fuh_HCHOdVCDprofile_2021to2023_warm_ppbv = PGN_fuh_HCHOdVCDprofile_2021to2023_warm_ppbv.map(lambda v: rename_second_dim(v, \"lev\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010351c0-5a67-4007-9d2e-afbfc8fd3d57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da57ce5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74a5274",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53ed66f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PGN_fus_HCHOVCD_2021to2023_warm_diurnal = {}\n",
    "GEOSCF_TOTCOL_HCHO_2021to2023_warm_diurnal = {}\n",
    "\n",
    "PGN_fus_HCHOVCD_2021to2023_warm_diurnal_std = {}\n",
    "GEOSCF_TOTCOL_HCHO_2021to2023_warm_diurnal_std = {}\n",
    "\n",
    "PGN_fus_HCHOVCD_2021to2023_warm_diurnal_count = {}\n",
    "GEOSCF_TOTCOL_HCHO_2021to2023_warm_diurnal_count = {}\n",
    "\n",
    "PGN_fus_HCHOVCD_2021to2023_warm_diurnal_2d = {}\n",
    "GEOSCF_TOTCOL_HCHO_2021to2023_warm_diurnal_2d = {}\n",
    "\n",
    "\n",
    "for i, (lat, lon, sitename) in enumerate(info_fush_union[:,0:3]):\n",
    "    \n",
    "    lat = float(lat)\n",
    "    lon = float(lon)\n",
    "\n",
    "\n",
    "    # Select the data for the current site from the datasets\n",
    "    geoscf_data = GEOSCF_TOTCOL_HCHO_2021to2023_warm[ sitename ] * 1e15\n",
    "\n",
    "    try:\n",
    "        pgn_data = PGN_fus_HCHOVCD_2021to2023_warm[ sitename ] * 6.02e19\n",
    "        pgn_flag = 0  # flag=0 means PGN is not all_nan\n",
    "    except:\n",
    "        pgn_data = PGN_fus_HCHOVCD_2021to2023_warm[ 'FairbanksAK_P174' ].copy()\n",
    "        pgn_data[~np.isnan(pgn_data)]=np.nan\n",
    "        pgn_flag = 1\n",
    "        \n",
    "    pgn_data = pgn_data.where(np.abs(pgn_data) <= 1e50)\n",
    "    geoscf_data, pgn_data = xr.align( geoscf_data, pgn_data, join='outer' )\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # Align time and nan values\n",
    "    nan_positions = np.isnan(pgn_data)\n",
    "    # Set NaN at those positions in each variable\n",
    "    pgn_data = pgn_data.where(~nan_positions, np.nan)\n",
    "    geoscf_data = geoscf_data.where(~nan_positions, np.nan)\n",
    "    # ------------------------------------------\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # calculate the diurnal variation\n",
    "    pgn_data_diurnal,pgn_data_diurnal_std,pgn_data_diurnal_count,pgn_data_diurnal_2d = calculate_diurnal_cycle_GC(pgn_data, lat, lon)[0:4]\n",
    "    geoscf_data_diurnal,geoscf_data_diurnal_std,geoscf_data_diurnal_count,geoscf_data_diurnal_2d = calculate_diurnal_cycle_GC(geoscf_data, lat, lon)[0:4]\n",
    "    \n",
    "    PGN_fus_HCHOVCD_2021to2023_warm_diurnal[sitename] = pgn_data_diurnal\n",
    "    GEOSCF_TOTCOL_HCHO_2021to2023_warm_diurnal[sitename] =  geoscf_data_diurnal\n",
    "\n",
    "    PGN_fus_HCHOVCD_2021to2023_warm_diurnal_std[sitename] = pgn_data_diurnal_std\n",
    "    GEOSCF_TOTCOL_HCHO_2021to2023_warm_diurnal_std[sitename] =  geoscf_data_diurnal_std\n",
    "\n",
    "    PGN_fus_HCHOVCD_2021to2023_warm_diurnal_count[sitename] = pgn_data_diurnal_count\n",
    "    GEOSCF_TOTCOL_HCHO_2021to2023_warm_diurnal_count[sitename] = geoscf_data_diurnal_count\n",
    "\n",
    "    PGN_fus_HCHOVCD_2021to2023_warm_diurnal_2d[sitename] = pgn_data_diurnal_2d\n",
    "    GEOSCF_TOTCOL_HCHO_2021to2023_warm_diurnal_2d[sitename] = geoscf_data_diurnal_2d\n",
    "\n",
    "\n",
    "PGN_fus_HCHOVCD_2021to2023_warm_diurnal = xr.Dataset(PGN_fus_HCHOVCD_2021to2023_warm_diurnal)\n",
    "GEOSCF_TOTCOL_HCHO_2021to2023_warm_diurnal = xr.Dataset(GEOSCF_TOTCOL_HCHO_2021to2023_warm_diurnal)\n",
    "\n",
    "PGN_fus_HCHOVCD_2021to2023_warm_diurnal_std = xr.Dataset(PGN_fus_HCHOVCD_2021to2023_warm_diurnal_std)\n",
    "GEOSCF_TOTCOL_HCHO_2021to2023_warm_diurnal_std = xr.Dataset(GEOSCF_TOTCOL_HCHO_2021to2023_warm_diurnal_std)\n",
    "\n",
    "PGN_fus_HCHOVCD_2021to2023_warm_diurnal_count = xr.Dataset(PGN_fus_HCHOVCD_2021to2023_warm_diurnal_count)\n",
    "GEOSCF_TOTCOL_HCHO_2021to2023_warm_diurnal_count = xr.Dataset(GEOSCF_TOTCOL_HCHO_2021to2023_warm_diurnal_count)\n",
    "\n",
    "PGN_fus_HCHOVCD_2021to2023_warm_diurnal_2d = xr.Dataset(PGN_fus_HCHOVCD_2021to2023_warm_diurnal_2d)\n",
    "GEOSCF_TOTCOL_HCHO_2021to2023_warm_diurnal_2d = xr.Dataset(GEOSCF_TOTCOL_HCHO_2021to2023_warm_diurnal_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "25144a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PGN_fuh_HCHOTropVCD_2021to2023_warm_diurnal = {}\n",
    "GEOSCF_TROPCOL_HCHO_2021to2023_warm_diurnal = {}\n",
    "\n",
    "PGN_fuh_HCHOTropVCD_2021to2023_warm_diurnal_std = {}\n",
    "GEOSCF_TROPCOL_HCHO_2021to2023_warm_diurnal_std = {}\n",
    "\n",
    "PGN_fuh_HCHOTropVCD_2021to2023_warm_diurnal_count = {}\n",
    "GEOSCF_TROPCOL_HCHO_2021to2023_warm_diurnal_count = {}\n",
    "\n",
    "PGN_fuh_HCHOTropVCD_2021to2023_warm_diurnal_2d = {}\n",
    "GEOSCF_TROPCOL_HCHO_2021to2023_warm_diurnal_2d = {}\n",
    "\n",
    "\n",
    "for i, (lat, lon, sitename) in enumerate(info_fush_union[:,0:3]):\n",
    "    \n",
    "    lat = float(lat)\n",
    "    lon = float(lon)\n",
    "\n",
    "\n",
    "    # Select the data for the current site from the datasets\n",
    "    geoscf_data = GEOSCF_TROPCOL_HCHO_2021to2023_warm[ sitename ]  * 1e15\n",
    "\n",
    "    try:\n",
    "        pgn_data = PGN_fuh_HCHOTropVCD_2021to2023_warm[ sitename ] * 6.02e19\n",
    "        pgn_flag = 0  # flag=0 means PGN is not all_nan\n",
    "    except:\n",
    "        pgn_data = PGN_fuh_HCHOTropVCD_2021to2023_warm[ 'FairbanksAK_P174' ].copy()\n",
    "        pgn_data[~np.isnan(pgn_data)]=np.nan\n",
    "        pgn_flag = 1\n",
    "\n",
    "    pgn_data = pgn_data.where(np.abs(pgn_data) <= 1e50)\n",
    "    geoscf_data, pgn_data = xr.align( geoscf_data, pgn_data, join='outer' )\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # Align time and nan values\n",
    "    nan_positions = np.isnan(pgn_data)\n",
    "\n",
    "    # Set NaN at those positions in each variable\n",
    "    pgn_data = pgn_data.where(~nan_positions, np.nan)\n",
    "    geoscf_data = geoscf_data.where(~nan_positions, np.nan)\n",
    "    # ------------------------------------------\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # calculate the diurnal variation\n",
    "    pgn_data_diurnal,pgn_data_diurnal_std, pgn_data_diurnal_count, pgn_data_diurnal_2d = calculate_diurnal_cycle_GC(pgn_data, lat, lon)[0:4]\n",
    "    geoscf_data_diurnal,geoscf_data_diurnal_std, geoscf_data_diurnal_count, geoscf_data_diurnal_2d = calculate_diurnal_cycle_GC(geoscf_data, lat, lon)[0:4]\n",
    "\n",
    "    \n",
    "    PGN_fuh_HCHOTropVCD_2021to2023_warm_diurnal[sitename] = pgn_data_diurnal\n",
    "    GEOSCF_TROPCOL_HCHO_2021to2023_warm_diurnal[sitename] =  geoscf_data_diurnal\n",
    "    \n",
    "    PGN_fuh_HCHOTropVCD_2021to2023_warm_diurnal_std[sitename] = pgn_data_diurnal_std\n",
    "    GEOSCF_TROPCOL_HCHO_2021to2023_warm_diurnal_std[sitename] =  geoscf_data_diurnal_std\n",
    "    \n",
    "    PGN_fuh_HCHOTropVCD_2021to2023_warm_diurnal_count[sitename] = pgn_data_diurnal_count\n",
    "    GEOSCF_TROPCOL_HCHO_2021to2023_warm_diurnal_count[sitename] =  geoscf_data_diurnal_count\n",
    "    \n",
    "    PGN_fuh_HCHOTropVCD_2021to2023_warm_diurnal_2d[sitename] = pgn_data_diurnal_2d\n",
    "    GEOSCF_TROPCOL_HCHO_2021to2023_warm_diurnal_2d[sitename] =  geoscf_data_diurnal_2d\n",
    "\n",
    "\n",
    "PGN_fuh_HCHOTropVCD_2021to2023_warm_diurnal = xr.Dataset(PGN_fuh_HCHOTropVCD_2021to2023_warm_diurnal)\n",
    "GEOSCF_TROPCOL_HCHO_2021to2023_warm_diurnal = xr.Dataset(GEOSCF_TROPCOL_HCHO_2021to2023_warm_diurnal)\n",
    "\n",
    "PGN_fuh_HCHOTropVCD_2021to2023_warm_diurnal_std = xr.Dataset(PGN_fuh_HCHOTropVCD_2021to2023_warm_diurnal_std)\n",
    "GEOSCF_TROPCOL_HCHO_2021to2023_warm_diurnal_std = xr.Dataset(GEOSCF_TROPCOL_HCHO_2021to2023_warm_diurnal_std)\n",
    "\n",
    "PGN_fuh_HCHOTropVCD_2021to2023_warm_diurnal_count = xr.Dataset(PGN_fuh_HCHOTropVCD_2021to2023_warm_diurnal_count)\n",
    "GEOSCF_TROPCOL_HCHO_2021to2023_warm_diurnal_count = xr.Dataset(GEOSCF_TROPCOL_HCHO_2021to2023_warm_diurnal_count)\n",
    "\n",
    "PGN_fuh_HCHOTropVCD_2021to2023_warm_diurnal_2d = xr.Dataset(PGN_fuh_HCHOTropVCD_2021to2023_warm_diurnal_2d)\n",
    "GEOSCF_TROPCOL_HCHO_2021to2023_warm_diurnal_2d = xr.Dataset(GEOSCF_TROPCOL_HCHO_2021to2023_warm_diurnal_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "250e0b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "PGN_fuh_HCHOSurfConc_2021to2023_warm_diurnal = {}\n",
    "GEOSCF_HCHO_2021to2023_warm_diurnal = {}\n",
    "PGN_fuh_HCHOSurfConc_2021to2023_warm_diurnal_std = {}\n",
    "GEOSCF_HCHO_2021to2023_warm_diurnal_std = {}\n",
    "PGN_fuh_HCHOSurfConc_2021to2023_warm_diurnal_count = {}\n",
    "GEOSCF_HCHO_2021to2023_warm_diurnal_count = {}\n",
    "PGN_fuh_HCHOSurfConc_2021to2023_warm_diurnal_2d = {}\n",
    "GEOSCF_HCHO_2021to2023_warm_diurnal_2d = {}\n",
    "\n",
    "for i, (lat, lon, sitename) in enumerate(info_fush_union[:,0:3]):\n",
    "    \n",
    "    lat = float(lat)\n",
    "    lon = float(lon)\n",
    "\n",
    "\n",
    "    # Select the data for the current site from the datasets\n",
    "    geoscf_data = GEOSCF_HCHO_2021to2023_warm[ sitename ]  * 1e9\n",
    "\n",
    "    try:\n",
    "        pgn_data = PGN_fuh_HCHOSurfConc_2021to2023_warm[ sitename ] * 22.4 / 1e3 * 1e9\n",
    "        pgn_flag = 0  # flag=0 means PGN is not all_nan\n",
    "    except:\n",
    "        pgn_data = geoscf_data.copy()\n",
    "        pgn_data[~np.isnan(pgn_data)]=np.nan\n",
    "        pgn_flag = 1\n",
    "\n",
    "    pgn_data = pgn_data.where(np.abs(pgn_data) <= 1e50)\n",
    "    geoscf_data, pgn_data = xr.align( geoscf_data, pgn_data, join='outer' )\n",
    "    # -------------------------------------------\n",
    "    # Align time and nan values\n",
    "#     if pgn_flag == 0:\n",
    "    nan_positions = np.isnan(pgn_data)\n",
    "    # Set NaN at those positions in each variable\n",
    "    pgn_data = pgn_data.where(~nan_positions, np.nan)\n",
    "    geoscf_data = geoscf_data.where(~nan_positions, np.nan)\n",
    "    # -------------------------------------------\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # calculate the diurnal variation\n",
    "    pgn_data_diurnal,pgn_data_diurnal_std, pgn_data_diurnal_count, pgn_data_diurnal_2d = calculate_diurnal_cycle_GC(pgn_data, lat, lon)[0:4]\n",
    "    geoscf_data_diurnal,geoscf_data_diurnal_std, geoscf_data_diurnal_count, geoscf_data_diurnal_2d = calculate_diurnal_cycle_GC(geoscf_data, lat, lon)[0:4]\n",
    "    \n",
    "    PGN_fuh_HCHOSurfConc_2021to2023_warm_diurnal[sitename] = pgn_data_diurnal\n",
    "    GEOSCF_HCHO_2021to2023_warm_diurnal[sitename] =  geoscf_data_diurnal\n",
    "\n",
    "    PGN_fuh_HCHOSurfConc_2021to2023_warm_diurnal_std[sitename] = pgn_data_diurnal_std\n",
    "    GEOSCF_HCHO_2021to2023_warm_diurnal_std[sitename] =  geoscf_data_diurnal_std\n",
    "    \n",
    "    PGN_fuh_HCHOSurfConc_2021to2023_warm_diurnal_count[sitename] = pgn_data_diurnal_count\n",
    "    GEOSCF_HCHO_2021to2023_warm_diurnal_count[sitename] =  geoscf_data_diurnal_count\n",
    "    \n",
    "    PGN_fuh_HCHOSurfConc_2021to2023_warm_diurnal_2d[sitename] = pgn_data_diurnal_2d\n",
    "    GEOSCF_HCHO_2021to2023_warm_diurnal_2d[sitename] =  geoscf_data_diurnal_2d\n",
    "\n",
    "\n",
    "PGN_fuh_HCHOSurfConc_2021to2023_warm_diurnal = xr.Dataset(PGN_fuh_HCHOSurfConc_2021to2023_warm_diurnal)\n",
    "GEOSCF_HCHO_2021to2023_warm_diurnal = xr.Dataset(GEOSCF_HCHO_2021to2023_warm_diurnal)\n",
    "\n",
    "PGN_fuh_HCHOSurfConc_2021to2023_warm_diurnal_std = xr.Dataset(PGN_fuh_HCHOSurfConc_2021to2023_warm_diurnal_std)\n",
    "GEOSCF_HCHO_2021to2023_warm_diurnal_std = xr.Dataset(GEOSCF_HCHO_2021to2023_warm_diurnal_std)\n",
    "\n",
    "PGN_fuh_HCHOSurfConc_2021to2023_warm_diurnal_count = xr.Dataset(PGN_fuh_HCHOSurfConc_2021to2023_warm_diurnal_count)\n",
    "GEOSCF_HCHO_2021to2023_warm_diurnal_count = xr.Dataset(GEOSCF_HCHO_2021to2023_warm_diurnal_count)\n",
    "\n",
    "PGN_fuh_HCHOSurfConc_2021to2023_warm_diurnal_2d = xr.Dataset(PGN_fuh_HCHOSurfConc_2021to2023_warm_diurnal_2d)\n",
    "GEOSCF_HCHO_2021to2023_warm_diurnal_2d = xr.Dataset(GEOSCF_HCHO_2021to2023_warm_diurnal_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c571d598",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n",
      "missed 1 day\n"
     ]
    }
   ],
   "source": [
    "PGN_fuh_HCHOdVCDprofile_2021to2023_warm_ppbv_diurnal = {}\n",
    "GEOSCF_HCHO_v36_2021to2023_warm_diurnal = {}\n",
    "# GC_SpeciesConc_CH2O_2021to2023_warm_vp_diurnal = {}\n",
    "PGN_fuh_HCHOdVCDprofile_2021to2023_warm_ppbv_diurnal_std = {}\n",
    "GEOSCF_HCHO_v36_2021to2023_warm_diurnal_std = {}\n",
    "# GC_SpeciesConc_CH2O_2021to2023_warm_vp_diurnal_std = {}\n",
    "PGN_fuh_HCHOdVCDprofile_2021to2023_warm_ppbv_diurnal_count = {}\n",
    "GEOSCF_HCHO_v36_2021to2023_warm_diurnal_count = {}\n",
    "# GC_SpeciesConc_CH2O_2021to2023_warm_vp_diurnal_count = {}\n",
    "PGN_fuh_HCHOdVCDprofile_2021to2023_warm_ppbv_diurnal_2d = {}\n",
    "GEOSCF_HCHO_v36_2021to2023_warm_diurnal_2d = {}\n",
    "# GC_SpeciesConc_CH2O_2021to2023_warm_vp_diurnal_2d = {}\n",
    "\n",
    "\n",
    "\n",
    "for i, (lat, lon, sitename) in enumerate(info_fush_union[:,0:3]):\n",
    "    \n",
    "    lat = float(lat)\n",
    "    lon = float(lon)\n",
    "\n",
    "        \n",
    "    # Select the data for the current site from the datasets\n",
    "    try:\n",
    "        geoscf_data = GEOSCF_HCHO_v36_2021to2023_warm[ sitename ]\n",
    "    except:\n",
    "        geoscf_data = GEOSCF_HCHO_v36_2021to2023_warm[ 'BronxNY_P180' ]\n",
    "        geoscf_data[~np.isnan(geoscf_data)]=np.nan\n",
    "        \n",
    "\n",
    "    try:\n",
    "        pgn_data = PGN_fuh_HCHOdVCDprofile_2021to2023_warm_ppbv[ sitename ]\n",
    "        pgn_flag = 0  # flag=0 means PGN is not all_nan\n",
    "    except:\n",
    "        pgn_data = PGN_fuh_HCHOdVCDprofile_2021to2023_warm_ppbv[ 'BronxNY_P180' ].copy()\n",
    "        pgn_data = pgn_data.where(np.isnan(pgn_data))\n",
    "        pgn_flag = 1\n",
    "\n",
    "    pgn_data = pgn_data.where(np.abs(pgn_data) <= 1e50)\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # Align time and nan values\n",
    "    pgn_data, geoscf_data = xr.align(pgn_data, geoscf_data, join='outer',\n",
    "                                    exclude=[dim for dim in pgn_data.dims if dim != 'time' ])\n",
    "    \n",
    "    pgn_data_mean = np.nanmean(pgn_data, axis=1)\n",
    "    nan_positions = np.where( np.isnan(pgn_data_mean) )[0]\n",
    "    \n",
    "    pgn_data[ nan_positions, : ] = np.nan\n",
    "    geoscf_data[ nan_positions, : ] = np.nan\n",
    "    \n",
    "    # -------------------------------------------\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # calculate the diurnal variation\n",
    "    pgn_data_diurnal,pgn_data_diurnal_std, pgn_data_diurnal_count, pgn_data_diurnal_2d = calculate_diurnal_cycle_GC(pgn_data, lat, lon)[0:4]\n",
    "    geoscf_data_diurnal,geoscf_data_diurnal_std, geoscf_data_diurnal_count, geoscf_data_diurnal_2d = calculate_diurnal_cycle_GC(geoscf_data, lat, lon)[0:4]\n",
    "#     gc_data_diurnal,gc_data_diurnal_std, gc_data_diurnal_count = calculate_diurnal_cycle_GC(gc_data, lat, lon)[0:3]\n",
    "    \n",
    "    PGN_fuh_HCHOdVCDprofile_2021to2023_warm_ppbv_diurnal[sitename] = pgn_data_diurnal\n",
    "#     GC_SpeciesConc_CH2O_2021to2023_warm_vp_diurnal[sitename] = gc_data_diurnal\n",
    "    GEOSCF_HCHO_v36_2021to2023_warm_diurnal[sitename] =  geoscf_data_diurnal\n",
    "\n",
    "    PGN_fuh_HCHOdVCDprofile_2021to2023_warm_ppbv_diurnal_std[sitename] = pgn_data_diurnal_std\n",
    "#     GC_SpeciesConc_CH2O_2021to2023_warm_vp_diurnal_std[sitename] = gc_data_diurnal_std\n",
    "    GEOSCF_HCHO_v36_2021to2023_warm_diurnal_std[sitename] =  geoscf_data_diurnal_std\n",
    "    \n",
    "    PGN_fuh_HCHOdVCDprofile_2021to2023_warm_ppbv_diurnal_count[sitename] = pgn_data_diurnal_count\n",
    "#     GC_SpeciesConc_CH2O_2021to2023_warm_vp_diurnal_count[sitename] = gc_data_diurnal_count\n",
    "    GEOSCF_HCHO_v36_2021to2023_warm_diurnal_count[sitename] =  geoscf_data_diurnal_count\n",
    "    \n",
    "    PGN_fuh_HCHOdVCDprofile_2021to2023_warm_ppbv_diurnal_2d[sitename] = pgn_data_diurnal_2d\n",
    "#     GC_SpeciesConc_CH2O_2021to2023_warm_vp_diurnal_count[sitename] = gc_data_diurnal_count\n",
    "    GEOSCF_HCHO_v36_2021to2023_warm_diurnal_2d[sitename] =  geoscf_data_diurnal_2d\n",
    "\n",
    "\n",
    "PGN_fuh_HCHOdVCDprofile_2021to2023_warm_ppbv_diurnal = xr.Dataset(PGN_fuh_HCHOdVCDprofile_2021to2023_warm_ppbv_diurnal)\n",
    "# GC_SpeciesConc_CH2O_2021to2023_warm_vp_diurnal = xr.Dataset(GC_SpeciesConc_CH2O_2021to2023_warm_vp_diurnal)\n",
    "GEOSCF_HCHO_v36_2021to2023_warm_diurnal = xr.Dataset(GEOSCF_HCHO_v36_2021to2023_warm_diurnal)\n",
    "PGN_fuh_HCHOdVCDprofile_2021to2023_warm_ppbv_diurnal_std = xr.Dataset(PGN_fuh_HCHOdVCDprofile_2021to2023_warm_ppbv_diurnal_std)\n",
    "# GC_SpeciesConc_CH2O_2021to2023_warm_vp_diurnal_std = xr.Dataset(GC_SpeciesConc_CH2O_2021to2023_warm_vp_diurnal_std)\n",
    "GEOSCF_HCHO_v36_2021to2023_warm_diurnal_std = xr.Dataset(GEOSCF_HCHO_v36_2021to2023_warm_diurnal_std)\n",
    "PGN_fuh_HCHOdVCDprofile_2021to2023_warm_ppbv_diurnal_count = xr.Dataset(PGN_fuh_HCHOdVCDprofile_2021to2023_warm_ppbv_diurnal_count)\n",
    "# GC_SpeciesConc_CH2O_2021to2023_warm_vp_diurnal_count = xr.Dataset(GC_SpeciesConc_CH2O_2021to2023_warm_vp_diurnal_count)\n",
    "GEOSCF_HCHO_v36_2021to2023_warm_diurnal_count = xr.Dataset(GEOSCF_HCHO_v36_2021to2023_warm_diurnal_count)\n",
    "PGN_fuh_HCHOdVCDprofile_2021to2023_warm_ppbv_diurnal_2d = xr.Dataset(PGN_fuh_HCHOdVCDprofile_2021to2023_warm_ppbv_diurnal_2d)\n",
    "# GC_SpeciesConc_CH2O_2021to2023_warm_vp_diurnal_2d = xr.Dataset(GC_SpeciesConc_CH2O_2021to2023_warm_vp_diurnal_2d)\n",
    "GEOSCF_HCHO_v36_2021to2023_warm_diurnal_2d = xr.Dataset(GEOSCF_HCHO_v36_2021to2023_warm_diurnal_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3353d196-5a54-479a-97bd-1c12562b7c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "PGN_fuh_profile_count_raw_df_2021to2023_warm_diurnal = {}\n",
    "PGN_fuh_profile_count_raw_df_2021to2023_warm_diurnal_std = {}\n",
    "PGN_fuh_profile_count_raw_df_2021to2023_warm_diurnal_count = {}\n",
    "PGN_fuh_profile_count_raw_df_2021to2023_warm_diurnal_2d = {}\n",
    "\n",
    "for i, (lat, lon, sitename) in enumerate(info_fush_union[:,0:3]):\n",
    "    \n",
    "    lat = float(lat)\n",
    "    lon = float(lon)\n",
    "\n",
    "    # Select the data for the current site from the datasets\n",
    "    geoscf_data = GEOSCF_HCHO_2021to2023_warm[ sitename ]\n",
    "\n",
    "    try:\n",
    "        pgn_data = PGN_fuh_profile_count_raw_df_2021to2023_warm[ sitename ]\n",
    "        pgn_flag = 0  # flag=0 means PGN is not all_nan\n",
    "    except:\n",
    "        pgn_data = geoscf_data.copy()\n",
    "        pgn_data[~np.isnan(pgn_data)]=np.nan\n",
    "        pgn_flag = 1\n",
    "\n",
    "    pgn_data = pgn_data.where(np.abs(pgn_data) <= 1e50)\n",
    "    geoscf_data, pgn_data = xr.align( geoscf_data, pgn_data, join='outer' )\n",
    "    # -------------------------------------------\n",
    "    # Align time and nan values\n",
    "#     if pgn_flag == 0:\n",
    "    nan_positions = np.isnan(pgn_data)\n",
    "    # Set NaN at those positions in each variable\n",
    "    pgn_data = pgn_data.where(~nan_positions, np.nan)\n",
    "    # -------------------------------------------\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # calculate the diurnal variation\n",
    "    pgn_data_diurnal,pgn_data_diurnal_std, pgn_data_diurnal_count, pgn_data_diurnal_2d = calculate_diurnal_cycle_GC(pgn_data, lat, lon)[0:4]\n",
    "     \n",
    "    PGN_fuh_profile_count_raw_df_2021to2023_warm_diurnal[sitename] = pgn_data_diurnal\n",
    "    PGN_fuh_profile_count_raw_df_2021to2023_warm_diurnal_2d[sitename] = pgn_data_diurnal_2d\n",
    "\n",
    "\n",
    "PGN_fuh_profile_count_raw_df_2021to2023_warm_diurnal = xr.Dataset(  PGN_fuh_profile_count_raw_df_2021to2023_warm_diurnal  )\n",
    "PGN_fuh_profile_count_raw_df_2021to2023_warm_diurnal_2d = xr.Dataset(  PGN_fuh_profile_count_raw_df_2021to2023_warm_diurnal_2d  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b12019eb-5032-4af4-8b23-d00c5beb7244",
   "metadata": {},
   "outputs": [],
   "source": [
    "PGN_fuh_profile_count_QC1_df_2021to2023_warm_diurnal = {}\n",
    "PGN_fuh_profile_count_QC1_df_2021to2023_warm_diurnal_std = {}\n",
    "PGN_fuh_profile_count_QC1_df_2021to2023_warm_diurnal_count = {}\n",
    "PGN_fuh_profile_count_QC1_df_2021to2023_warm_diurnal_2d = {}\n",
    "\n",
    "for i, (lat, lon, sitename) in enumerate(info_fush_union[:,0:3]):\n",
    "    \n",
    "    lat = float(lat)\n",
    "    lon = float(lon)\n",
    "\n",
    "    # Select the data for the current site from the datasets\n",
    "    geoscf_data = GEOSCF_HCHO_2021to2023_warm[ sitename ]\n",
    "\n",
    "    try:\n",
    "        pgn_data = PGN_fuh_profile_count_QC1_df_2021to2023_warm[ sitename ]\n",
    "        pgn_flag = 0  # flag=0 means PGN is not all_nan\n",
    "    except:\n",
    "        pgn_data = geoscf_data.copy()\n",
    "        pgn_data[~np.isnan(pgn_data)]=np.nan\n",
    "        pgn_flag = 1\n",
    "\n",
    "    pgn_data = pgn_data.where(np.abs(pgn_data) <= 1e50)\n",
    "    geoscf_data, pgn_data = xr.align( geoscf_data, pgn_data, join='outer' )\n",
    "    # -------------------------------------------\n",
    "    # Align time and nan values\n",
    "#     if pgn_flag == 0:\n",
    "    nan_positions = np.isnan(pgn_data)\n",
    "    # Set NaN at those positions in each variable\n",
    "    pgn_data = pgn_data.where(~nan_positions, np.nan)\n",
    "    # -------------------------------------------\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # calculate the diurnal variation\n",
    "    pgn_data_diurnal,pgn_data_diurnal_std, pgn_data_diurnal_count, pgn_data_diurnal_2d = calculate_diurnal_cycle_GC(pgn_data, lat, lon)[0:4]\n",
    "     \n",
    "    PGN_fuh_profile_count_QC1_df_2021to2023_warm_diurnal[sitename] = pgn_data_diurnal\n",
    "    PGN_fuh_profile_count_QC1_df_2021to2023_warm_diurnal_2d[sitename] = pgn_data_diurnal_2d\n",
    "\n",
    "\n",
    "PGN_fuh_profile_count_QC1_df_2021to2023_warm_diurnal = xr.Dataset(  PGN_fuh_profile_count_QC1_df_2021to2023_warm_diurnal  )\n",
    "PGN_fuh_profile_count_QC1_df_2021to2023_warm_diurnal_2d = xr.Dataset(  PGN_fuh_profile_count_QC1_df_2021to2023_warm_diurnal_2d  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "dba4af45-5f42-48b0-a5e0-03fdd9744bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PGN_fuh_profile_count_QC2_df_2021to2023_warm_diurnal = {}\n",
    "PGN_fuh_profile_count_QC2_df_2021to2023_warm_diurnal_std = {}\n",
    "PGN_fuh_profile_count_QC2_df_2021to2023_warm_diurnal_count = {}\n",
    "PGN_fuh_profile_count_QC2_df_2021to2023_warm_diurnal_2d = {}\n",
    "\n",
    "for i, (lat, lon, sitename) in enumerate(info_fush_union[:,0:3]):\n",
    "    \n",
    "    lat = float(lat)\n",
    "    lon = float(lon)\n",
    "\n",
    "    # Select the data for the current site from the datasets\n",
    "    geoscf_data = GEOSCF_HCHO_2021to2023_warm[ sitename ]\n",
    "\n",
    "    try:\n",
    "        pgn_data = PGN_fuh_profile_count_QC2_df_2021to2023_warm[ sitename ]\n",
    "        pgn_flag = 0  # flag=0 means PGN is not all_nan\n",
    "    except:\n",
    "        pgn_data = geoscf_data.copy()\n",
    "        pgn_data[~np.isnan(pgn_data)]=np.nan\n",
    "        pgn_flag = 1\n",
    "\n",
    "    pgn_data = pgn_data.where(np.abs(pgn_data) <= 1e50)\n",
    "    geoscf_data, pgn_data = xr.align( geoscf_data, pgn_data, join='outer' )\n",
    "    # -------------------------------------------\n",
    "    # Align time and nan values\n",
    "#     if pgn_flag == 0:\n",
    "    nan_positions = np.isnan(pgn_data)\n",
    "    # Set NaN at those positions in each variable\n",
    "    pgn_data = pgn_data.where(~nan_positions, np.nan)\n",
    "    # -------------------------------------------\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # calculate the diurnal variation\n",
    "    pgn_data_diurnal,pgn_data_diurnal_std, pgn_data_diurnal_count, pgn_data_diurnal_2d = calculate_diurnal_cycle_GC(pgn_data, lat, lon)[0:4]\n",
    "     \n",
    "    PGN_fuh_profile_count_QC2_df_2021to2023_warm_diurnal[sitename] = pgn_data_diurnal\n",
    "    PGN_fuh_profile_count_QC2_df_2021to2023_warm_diurnal_2d[sitename] = pgn_data_diurnal_2d\n",
    "\n",
    "\n",
    "PGN_fuh_profile_count_QC2_df_2021to2023_warm_diurnal = xr.Dataset(  PGN_fuh_profile_count_QC2_df_2021to2023_warm_diurnal  )\n",
    "PGN_fuh_profile_count_QC2_df_2021to2023_warm_diurnal_2d = xr.Dataset(  PGN_fuh_profile_count_QC2_df_2021to2023_warm_diurnal_2d  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba574f9-fcf2-4299-85a9-00a7f1dff392",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c7081a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "PGN_fuh_HCHOdVCDprofile_2021to2023_warm_dVCD_diurnal = {}\n",
    "PGN_fuh_HCHOdVCDprofile_2021to2023_warm_dVCD_diurnal_std = {}\n",
    "PGN_fuh_HCHOdVCDprofile_2021to2023_warm_dVCD_diurnal_count = {}\n",
    "PGN_fuh_HCHOdVCDprofile_2021to2023_warm_dVCD_diurnal_2d = {}\n",
    "\n",
    "\n",
    "\n",
    "for i, (lat, lon, sitename) in enumerate(info_fush_union[:,0:3]):\n",
    "    \n",
    "    lat = float(lat)\n",
    "    lon = float(lon)\n",
    "\n",
    "        \n",
    "    # Select the data for the current site from the datasets\n",
    "    try:\n",
    "        geoscf_data = GEOSCF_HCHO_v36_2021to2023_warm[ sitename ]\n",
    "    except:\n",
    "        geoscf_data = GEOSCF_HCHO_v36_2021to2023_warm[ 'BronxNY_P180' ]\n",
    "        geoscf_data[~np.isnan(geoscf_data)]=np.nan\n",
    "        \n",
    "\n",
    "    try:\n",
    "        pgn_data = PGN_fuh_HCHOdVCDprofile_2021to2023_warm[ sitename ] * 250 *1e2\n",
    "        pgn_flag = 0  # flag=0 means PGN is not all_nan\n",
    "    except:\n",
    "        pgn_data = PGN_fuh_HCHOdVCDprofile_2021to2023_warm[ 'BronxNY_P180' ].copy() * 250 *1e2\n",
    "        pgn_data = pgn_data.where(np.isnan(pgn_data))\n",
    "        pgn_flag = 1\n",
    "\n",
    "    pgn_data = pgn_data.where(np.abs(pgn_data) <= 1e50)\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # Align time and nan values\n",
    "    pgn_data, geoscf_data = xr.align(pgn_data, geoscf_data, join='outer',\n",
    "                                    exclude=[dim for dim in pgn_data.dims if dim != 'time' ])\n",
    "    try:\n",
    "        pgn_data_mean = np.nanmean(pgn_data, axis=1)\n",
    "        nan_positions = np.where( np.isnan(pgn_data_mean) )[0]\n",
    "        \n",
    "        pgn_data[ nan_positions, : ] = np.nan\n",
    "        geoscf_data[ nan_positions, : ] = np.nan\n",
    "\n",
    "    except:\n",
    "        pgn_data = geoscf_data.copy()\n",
    "        pgn_data = pgn_data.where(np.isnan(pgn_data))\n",
    "        \n",
    "    \n",
    "    # -------------------------------------------\n",
    "\n",
    "    # -------------------------------------------\n",
    "    # calculate the diurnal variation\n",
    "    pgn_data_diurnal,pgn_data_diurnal_std, pgn_data_diurnal_count, pgn_data_diurnal_2d = calculate_diurnal_cycle_GC(pgn_data, lat, lon)[0:4]\n",
    "    \n",
    "    PGN_fuh_HCHOdVCDprofile_2021to2023_warm_dVCD_diurnal[sitename] = pgn_data_diurnal\n",
    "    PGN_fuh_HCHOdVCDprofile_2021to2023_warm_dVCD_diurnal_std[sitename] = pgn_data_diurnal_std\n",
    "    PGN_fuh_HCHOdVCDprofile_2021to2023_warm_dVCD_diurnal_count[sitename] = pgn_data_diurnal_count\n",
    "    PGN_fuh_HCHOdVCDprofile_2021to2023_warm_dVCD_diurnal_2d[sitename] = pgn_data_diurnal_2d\n",
    "\n",
    "\n",
    "PGN_fuh_HCHOdVCDprofile_2021to2023_warm_dVCD_diurnal = xr.Dataset(PGN_fuh_HCHOdVCDprofile_2021to2023_warm_dVCD_diurnal)\n",
    "PGN_fuh_HCHOdVCDprofile_2021to2023_warm_dVCD_diurnal_std = xr.Dataset(PGN_fuh_HCHOdVCDprofile_2021to2023_warm_dVCD_diurnal_std)\n",
    "PGN_fuh_HCHOdVCDprofile_2021to2023_warm_dVCD_diurnal_count = xr.Dataset(PGN_fuh_HCHOdVCDprofile_2021to2023_warm_dVCD_diurnal_count)\n",
    "PGN_fuh_HCHOdVCDprofile_2021to2023_warm_dVCD_diurnal_2d = xr.Dataset(PGN_fuh_HCHOdVCDprofile_2021to2023_warm_dVCD_diurnal_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a70bc6e1-362a-405f-931c-88223c8aa2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_lev_vars_with_nan_and_adjust_dims(dataset):\n",
    "    \"\"\"\n",
    "    找出所有依赖 'lev' 这个坐标的变量：\n",
    "    - 用全 NaN 替换它们\n",
    "    - 重新调整它们的坐标结构，使其与不含 'lev' 的变量一致\n",
    "    - 最后删除 'lev' 坐标\n",
    "    \n",
    "    Parameters:\n",
    "        dataset (xr.Dataset): 需要处理的 xarray 数据集\n",
    "    \n",
    "    Returns:\n",
    "        xr.Dataset: 处理后的数据集\n",
    "    \"\"\"\n",
    "    # 复制 dataset，避免修改原数据\n",
    "    dataset = dataset.copy()\n",
    "    \n",
    "    # 找出所有使用 'lev' 作为维度的变量\n",
    "    vars_with_lev = [var for var in dataset.data_vars if 'lev' in dataset[var].dims]\n",
    "\n",
    "    # 获取 dataset 中其他变量的主要坐标维度\n",
    "    other_vars = [var for var in dataset.data_vars if var not in vars_with_lev]\n",
    "    if other_vars:\n",
    "        reference_dims = dataset[other_vars[0]].dims  # 选一个没有 'lev' 的变量作为参考\n",
    "    else:\n",
    "        reference_dims = tuple(dim for dim in dataset.dims if dim != \"lev\")  # 如果所有变量都有 lev\n",
    "\n",
    "    # 替换这些变量为全 NaN，并调整维度\n",
    "    for var in vars_with_lev:\n",
    "        shape = tuple(dataset.sizes[dim] for dim in reference_dims)  # 参考其他变量的 shape\n",
    "        dataset[var] = (reference_dims, np.full(shape, np.nan))  # 赋值为全 NaN 并修改维度\n",
    "\n",
    "    # 删除 'lev' 这个坐标（如果还存在）\n",
    "    if 'lev' in dataset.coords:\n",
    "        dataset = dataset.drop_vars('lev')\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "PGN_fuh_HCHOdVCDprofile_2021to2023_warm_dVCD_diurnal = replace_lev_vars_with_nan_and_adjust_dims(  PGN_fuh_HCHOdVCDprofile_2021to2023_warm_dVCD_diurnal)\n",
    "PGN_fuh_HCHOdVCDprofile_2021to2023_warm_dVCD_diurnal_std = replace_lev_vars_with_nan_and_adjust_dims(  PGN_fuh_HCHOdVCDprofile_2021to2023_warm_dVCD_diurnal_std)\n",
    "PGN_fuh_HCHOdVCDprofile_2021to2023_warm_dVCD_diurnal_count = replace_lev_vars_with_nan_and_adjust_dims(  PGN_fuh_HCHOdVCDprofile_2021to2023_warm_dVCD_diurnal_count)\n",
    "PGN_fuh_HCHOdVCDprofile_2021to2023_warm_dVCD_diurnal_2d = replace_lev_vars_with_nan_and_adjust_dims(  PGN_fuh_HCHOdVCDprofile_2021to2023_warm_dVCD_diurnal_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f110ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a22254f-5565-47c9-8e49-7800875a8836",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed1e639-c994-458c-980e-6fa9087a5c18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "865f5b5a-9b94-4157-8621-c13739bd8f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to netcdf files for the processed diurnal data \n",
    "processed_variable_names = [\n",
    "    # \"GEOSCF_HCHO_2021to2023_warm\",\n",
    "    # \"GEOSCF_TROPCOL_HCHO_2021to2023_warm\",\n",
    "    # \"GEOSCF_TOTCOL_HCHO_2021to2023_warm\",\n",
    "    # # \"GEOSCF_HCHO_v36_2021to2023_warm\",\n",
    "    # \"PGN_fuh_HCHOTropVCD_2021to2023_warm\",\n",
    "    # \"PGN_fuh_HCHOSurfConc_2021to2023_warm\",\n",
    "    # # \"PGN_fuh_HCHOdVCDprofile_2021to2023_warm\",\n",
    "    # \"PGN_fus_HCHOVCD_2021to2023_warm\",\n",
    "    # # \"GEOSCF_T_v36_2021to2023_warm\",\n",
    "    # # \"PGN_fuh_HCHOdVCDprofile_2021to2023_warm_ppbv\",\n",
    "    # # \"PGN_fuh_HCHOdVCDprofile_2021to2023_warm_ppbv\",\n",
    "    \n",
    "    # \"PGN_fus_HCHOVCD_2021to2023_warm_diurnal\",\n",
    "    # \"GEOSCF_TOTCOL_HCHO_2021to2023_warm_diurnal\",\n",
    "    # \"PGN_fus_HCHOVCD_2021to2023_warm_diurnal_std\",\n",
    "    # \"GEOSCF_TOTCOL_HCHO_2021to2023_warm_diurnal_std\",\n",
    "    # \"PGN_fus_HCHOVCD_2021to2023_warm_diurnal_count\",\n",
    "    # \"GEOSCF_TOTCOL_HCHO_2021to2023_warm_diurnal_count\",\n",
    "    # \"PGN_fus_HCHOVCD_2021to2023_warm_diurnal_2d\",\n",
    "    # \"GEOSCF_TOTCOL_HCHO_2021to2023_warm_diurnal_2d\",\n",
    "\n",
    "    \"PGN_fuh_HCHOTropVCD_2021to2023_warm\",\n",
    "    \"GEOSCF_TROPCOL_HCHO_2021to2023_warm\",\n",
    "    \"PGN_fuh_HCHOTropVCD_2021to2023_warm_diurnal\",\n",
    "    \"GEOSCF_TROPCOL_HCHO_2021to2023_warm_diurnal\",\n",
    "    \"PGN_fuh_HCHOTropVCD_2021to2023_warm_diurnal_std\",\n",
    "    \"GEOSCF_TROPCOL_HCHO_2021to2023_warm_diurnal_std\",\n",
    "    \"PGN_fuh_HCHOTropVCD_2021to2023_warm_diurnal_count\",\n",
    "    \"GEOSCF_TROPCOL_HCHO_2021to2023_warm_diurnal_count\",\n",
    "    \"PGN_fuh_HCHOTropVCD_2021to2023_warm_diurnal_2d\",\n",
    "    \"GEOSCF_TROPCOL_HCHO_2021to2023_warm_diurnal_2d\",\n",
    "\n",
    "    # \"PGN_fuh_HCHOSurfConc_2021to2023_warm\",\n",
    "    # \"GEOSCF_HCHO_2021to2023_warm\",\n",
    "    # \"PGN_fuh_HCHOSurfConc_2021to2023_warm_diurnal\",\n",
    "    # \"GEOSCF_HCHO_2021to2023_warm_diurnal\",\n",
    "    # \"PGN_fuh_HCHOSurfConc_2021to2023_warm_diurnal_std\",\n",
    "    # \"GEOSCF_HCHO_2021to2023_warm_diurnal_std\",\n",
    "    # \"PGN_fuh_HCHOSurfConc_2021to2023_warm_diurnal_count\",\n",
    "    # \"GEOSCF_HCHO_2021to2023_warm_diurnal_count\",\n",
    "    # \"PGN_fuh_HCHOSurfConc_2021to2023_warm_diurnal_2d\",\n",
    "    # \"GEOSCF_HCHO_2021to2023_warm_diurnal_2d\",\n",
    "\n",
    "    # \"GEOSCF_HCHO_v36_2021to2023_warm\",\n",
    "    # \"PGN_fuh_HCHOdVCDprofile_2021to2023_warm_ppbv\",\n",
    "    # \"PGN_fuh_HCHOdVCDprofile_2021to2023_warm_ppbv_diurnal\",\n",
    "    # \"GEOSCF_HCHO_v36_2021to2023_warm_diurnal\",\n",
    "    # \"PGN_fuh_HCHOdVCDprofile_2021to2023_warm_ppbv_diurnal_std\",\n",
    "    # \"GEOSCF_HCHO_v36_2021to2023_warm_diurnal_std\",\n",
    "    # \"PGN_fuh_HCHOdVCDprofile_2021to2023_warm_ppbv_diurnal_count\",\n",
    "    # \"GEOSCF_HCHO_v36_2021to2023_warm_diurnal_count\",\n",
    "    # \"PGN_fuh_HCHOdVCDprofile_2021to2023_warm_ppbv_diurnal_2d\",\n",
    "    # \"GEOSCF_HCHO_v36_2021to2023_warm_diurnal_2d\",\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "# processed_variable_names = [\n",
    "\n",
    "#     \"GEOSCF_HCHO_v36_2021to2023_warm\",\n",
    "#     \"PGN_fuh_HCHOdVCDprofile_2021to2023_warm\",\n",
    "#     # \"GEOSCF_T_v36_2021to2023_warm\",\n",
    "#     \"PGN_fuh_HCHOdVCDprofile_2021to2023_warm_ppbv\",\n",
    "#     \"PGN_fuh_HCHOdVCDprofile_2021to2023_warm_ppbv_diurnal\",\n",
    "#     \"GEOSCF_HCHO_v36_2021to2023_warm_diurnal\",\n",
    "#     \"PGN_fuh_HCHOdVCDprofile_2021to2023_warm_ppbv_diurnal_std\",\n",
    "#     \"GEOSCF_HCHO_v36_2021to2023_warm_diurnal_std\",\n",
    "#     \"PGN_fuh_HCHOdVCDprofile_2021to2023_warm_ppbv_diurnal_count\",\n",
    "#     \"GEOSCF_HCHO_v36_2021to2023_warm_diurnal_count\",\n",
    "#     \"PGN_fuh_HCHOdVCDprofile_2021to2023_warm_ppbv_diurnal_2d\",\n",
    "#     \"GEOSCF_HCHO_v36_2021to2023_warm_diurnal_2d\",\n",
    "\n",
    "# ]\n",
    "\n",
    "\n",
    "processed_variable_names = [\n",
    "    \"PGN_fuh_HCHOSurfConc_2021to2023_warm\",\n",
    "    \"GEOSCF_HCHO_2021to2023_warm\",\n",
    "    \"PGN_fuh_HCHOSurfConc_2021to2023_warm_diurnal\",\n",
    "    \"GEOSCF_HCHO_2021to2023_warm_diurnal\",\n",
    "    \"PGN_fuh_HCHOSurfConc_2021to2023_warm_diurnal_std\",\n",
    "    \"GEOSCF_HCHO_2021to2023_warm_diurnal_std\",\n",
    "    \"PGN_fuh_HCHOSurfConc_2021to2023_warm_diurnal_count\",\n",
    "    \"GEOSCF_HCHO_2021to2023_warm_diurnal_count\",\n",
    "    \"PGN_fuh_HCHOSurfConc_2021to2023_warm_diurnal_2d\",\n",
    "    \"GEOSCF_HCHO_2021to2023_warm_diurnal_2d\",\n",
    "]\n",
    "\n",
    "\n",
    "# processed_variable_names = [\n",
    "#     \"PGN_fuh_HCHOdVCDprofile_2021to2023_warm_dVCD_diurnal\",\n",
    "#     \"PGN_fuh_HCHOdVCDprofile_2021to2023_warm_dVCD_diurnal_std\",\n",
    "#     \"PGN_fuh_HCHOdVCDprofile_2021to2023_warm_dVCD_diurnal_count\",\n",
    "#     \"PGN_fuh_HCHOdVCDprofile_2021to2023_warm_dVCD_diurnal_2d\",\n",
    "# ]\n",
    "\n",
    "\n",
    "processed_variable_names = [\n",
    "    \"PGN_fuh_profile_count_raw_df_2021to2023_warm_diurnal\",\n",
    "    \"PGN_fuh_profile_count_raw_df_2021to2023_warm_diurnal_2d\",\n",
    "    \"PGN_fuh_profile_count_QC1_df_2021to2023_warm_diurnal\",\n",
    "    \"PGN_fuh_profile_count_QC1_df_2021to2023_warm_diurnal_2d\",\n",
    "    \"PGN_fuh_profile_count_QC2_df_2021to2023_warm_diurnal\",\n",
    "    \"PGN_fuh_profile_count_QC2_df_2021to2023_warm_diurnal_2d\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "cb83029f-a573-4e28-afd3-c1748226e524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to /import/GREENING/tzhao/jndata/paper4_data/PGN_fuh_profile_count_raw_df_2021to2023_warm_diurnal.nc\n",
      "Saved to /import/GREENING/tzhao/jndata/paper4_data/PGN_fuh_profile_count_raw_df_2021to2023_warm_diurnal_2d.nc\n",
      "Saved to /import/GREENING/tzhao/jndata/paper4_data/PGN_fuh_profile_count_QC1_df_2021to2023_warm_diurnal.nc\n",
      "Saved to /import/GREENING/tzhao/jndata/paper4_data/PGN_fuh_profile_count_QC1_df_2021to2023_warm_diurnal_2d.nc\n",
      "Saved to /import/GREENING/tzhao/jndata/paper4_data/PGN_fuh_profile_count_QC2_df_2021to2023_warm_diurnal.nc\n",
      "Saved to /import/GREENING/tzhao/jndata/paper4_data/PGN_fuh_profile_count_QC2_df_2021to2023_warm_diurnal_2d.nc\n"
     ]
    }
   ],
   "source": [
    "# Loop over each variable name\n",
    "for varname in processed_variable_names:\n",
    "    # Access the variable directly from memory\n",
    "    data = locals()[varname]\n",
    "\n",
    "    # Save the dataset or data array to a .nc file\n",
    "    # Replace `path_to_save` with the directory where you want to save the .nc files\n",
    "    data.to_netcdf(f\"/import/GREENING/tzhao/jndata/paper4_data/{varname}.nc\", engine='netcdf4')\n",
    "    print(f\"Saved to /import/GREENING/tzhao/jndata/paper4_data/{varname}.nc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7fced3-21e6-4bf6-a9d1-54c1b5850775",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a237bf82-5db2-4b15-b16d-627397cf532b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
